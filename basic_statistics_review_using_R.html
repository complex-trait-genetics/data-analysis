<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>basic_statistics_review_using_R.utf8.md</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/lumen.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 54px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h2 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h3 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h4 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h5 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h6 {
  padding-top: 59px;
  margin-top: -59px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Data Analysis</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="probability_theory.html">Probability theory</a>
</li>
<li>
  <a href="basic_statistics_review_using_R.html">Basic statistics</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<div id="arosur" class="section level1">
<h1><span class="header-section-number">1</span> A Review of Statistics using R</h1>
<p>This second section reviews important statistical concepts that are heavily used in biology:</p>
<ul>
<li><p>Estimation of unknown population parameters</p></li>
<li><p>Hypothesis testing</p></li>
<li><p>Confidence intervals</p></li>
<li><p>Linear regression with one variable</p></li>
</ul>
<p>We will discuss the first three in the simple context of inference about an unknown population mean and discuss several applications in <tt>R</tt>. These <tt>R</tt> applications rely on the following packages which are not part of the base version of <tt>R</tt>:</p>
<ul>
<li><p><tt>readxl</tt> - allows to import data from <em>Excel</em> to <tt>R</tt>.</p></li>
<li><p><tt>dplyr</tt> - provides a flexible grammar for data manipulation.</p></li>
<li><p><tt>MASS</tt> - a collection of functions for applied statistics.</p></li>
</ul>
<p>Make sure these are installed before you go ahead and try to replicate the examples. The safest way to do so is by checking whether the following code chunk executes without any errors.</p>
<pre class="r"><code>library(dplyr)
library(MASS)
library(readxl)</code></pre>
<p>We will then cover the basics of linear regression and show how to perform regression analysis in <tt>R</tt>. In linear regression, the aim is to model the relationship between a dependent variable <span class="math inline">\(Y\)</span> and one or more explanatory variables denoted by <span class="math inline">\(X_1, X_2, \dots, X_k\)</span>. In simple linear regression, that we will focus on first, there is just one explanatory variable <span class="math inline">\(X_1\)</span>. <br></p>
<p>If, for example, if we have a single nucleotide genetic marker <span class="math inline">\(X_1\)</span>, we would like to know how changing the base-pair would affect <span class="math inline">\(Y\)</span>, the height of individuals? With linear regression we can not only examine whether the marker <em>does have</em> an impact on height, but we can also learn about the <em>direction</em> and the <em>strength</em> of this effect.</p>

<div id="estimation-of-the-population-mean" class="section level2">
<h2><span class="header-section-number">1.1</span> Estimation of the Population Mean</h2>
<div id="KC8" class="keyconcept">
<h3 class="right">
Key Concept 8
</h3>
<h3 class="left">
Estimators and Estimates
</h3>
<p><em>Estimators</em> are functions of sample data drawn from an unknown population. <em>Estimates</em> are numeric values computed by estimators based on the sample data. Estimators are random variables because they are functions of <em>random</em> data. Estimates are nonrandom numbers.</p>
</div>
<p>Think of some variable, let's say hourly income in Switzerland, denoted by <span class="math inline">\(Y\)</span>. Suppose we are interested in <span class="math inline">\(\mu_Y\)</span> the mean of <span class="math inline">\(Y\)</span>. In order to exactly calculate <span class="math inline">\(\mu_Y\)</span> we would have to determine the hourly income of every individual in Switzerland today. We simply cannot do this due to time and cost constraints. However, we can draw a random sample of <span class="math inline">\(n\)</span> i.i.d. observations <span class="math inline">\(Y_1, \dots, Y_n\)</span> and estimate <span class="math inline">\(\mu_Y\)</span> using one of the simplest estimators one can think of, that is,</p>
<p><span class="math display">\[ \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i, \]</span></p>
<p>the sample mean of <span class="math inline">\(Y\)</span>. Then again, we could use an even simpler estimator for <span class="math inline">\(\mu_Y\)</span>: the very first observation in the sample, <span class="math inline">\(Y_1\)</span>. Is <span class="math inline">\(Y_1\)</span> a good estimator? For now, assume that</p>
<p><span class="math display">\[ Y \sim \chi_{12}^2 \]</span></p>
<p>which is not too unreasonable as hourly income is non-negative and we expect many hourly earnings to be in a range of <span class="math inline">\(5€\,\)</span> to <span class="math inline">\(15€\)</span>. Moreover, it is common for income distributions to be skewed to the right — a property of the <span class="math inline">\(\chi^2_{12}\)</span> distribution.</p>
<div class="unfolded">
<pre class="r"><code># plot the chi_12^2 distribution
curve(dchisq(x, df=12), 
      from = 0, 
      to = 40, 
      ylab = &quot;density&quot;, 
      xlab = &quot;hourly earnings in Euro&quot;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-5-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>We now draw a sample of <span class="math inline">\(n=100\)</span> observations and take the first observation <span class="math inline">\(Y_1\)</span> as an estimate for <span class="math inline">\(\mu_Y\)</span></p>
<pre class="r"><code># set seed for reproducibility
set.seed(1)
# sample from the chi_12^2 distribution, use only the first observation
rsamp &lt;- rchisq(n = 100, df = 12)
rsamp[1]</code></pre>
<pre><code>## [1] 8.257893</code></pre>
<p>The estimate <span class="math inline">\(8.26\)</span> is not too far away from <span class="math inline">\(\mu_Y = 12\)</span> but it is somewhat intuitive that we could do better: the estimator <span class="math inline">\(Y_1\)</span> discards a lot of information and its variance is the population variance:</p>
<p><span class="math display">\[ \text{Var}(Y_1) = \text{Var}(Y) = 2 \cdot 12 = 24 \]</span></p>
<p>This brings us to the following question: What is a <em>good</em> estimator of an unknown parameter in the first place? This question is tackled in Key Concept 9.</p>
<div id="KC9" class="keyconcept">
<h3 class="right">
Key Concept 9
</h3>
<h3 class="left">
Bias, Consistency and Efficiency
</h3>
<p>Desirable characteristics of an estimator include unbiasedness, consistency and efficiency.</p>
<p><strong>Unbiasedness:</strong><br /></p>
<p>If the mean of the sampling distribution of some estimator <span class="math inline">\(\hat\mu_Y\)</span> for the population mean <span class="math inline">\(\mu_Y\)</span> equals <span class="math inline">\(\mu_Y\)</span>, <span class="math display">\[ E(\hat\mu_Y) = \mu_Y, \]</span> the estimator is unbiased for <span class="math inline">\(\mu_Y\)</span>.</p> 
<p>The <em>bias</em> of <span class="math inline">\(\hat\mu_Y\)</span> then is <span class="math inline">\(0\)</span>: <span class="math display">\[ E(\hat\mu_Y) - \mu_Y = 0\]</span> <strong>Consistency:</strong></p> 
<p>We want the uncertainty of the estimator <span class="math inline">\(\mu_Y\)</span> to decrease as the number of observations in the sample grows. More precisely, we want the probability that the estimate <span class="math inline">\(\hat\mu_Y\)</span> falls within a small interval around the true value <span class="math inline">\(\mu_Y\)</span> to get increasingly closer to <span class="math inline">\(1\)</span> as <span class="math inline">\(n\)</span> grows.</p>
<p>We write this as <span class="math display">\[ \hat\mu_Y \xrightarrow{p} \mu_Y. \]</span></p>
<p><strong>Variance and efficiency:</strong> We want the estimator to be efficient. Suppose we have two estimators, <span class="math inline">\(\hat\mu_Y\)</span> and <span class="math inline">\(\overset{\sim}{\mu}_Y\)</span> and for some given sample size <span class="math inline">\(n\)</span> it holds that <span class="math display">\[ E(\hat\mu_Y) = E(\overset{\sim}{\mu}_Y) = \mu_Y \]</span> but <span class="math display">\[\text{Var}(\hat\mu_Y) &lt; \text{Var}(\overset{\sim}{\mu}_Y).\]</span> We then prefer to use <span class="math inline">\(\hat\mu_Y\)</span> as it has a lower variance than <span class="math inline">\(\overset{\sim}{\mu}_Y\)</span>, meaning that <span class="math inline">\(\hat\mu_Y\)</span> is more <em>efficient</em> in using the information provided by the observations in the sample.</p>
</div>
</div>
<div id="potsm" class="section level2">
<h2><span class="header-section-number">1.2</span> Properties of the Sample Mean</h2>

<p>A more precise way to express consistency of an estimator <span class="math inline">\(\hat\mu\)</span> for a parameter <span class="math inline">\(\mu\)</span> is <span class="math display">\[ P(|\hat{\mu} - \mu|&lt;\epsilon) \xrightarrow[n \rightarrow \infty]{p} 1 \quad \text{for any}\quad\epsilon&gt;0.\]</span>.</p> 
<p>This expression says that the probability of observing a deviation from the true value <span class="math inline">\(\mu\)</span> that is smaller than some arbitrary <span class="math inline">\(\epsilon &gt; 0\)</span> converges to <span class="math inline">\(1\)</span> as <span class="math inline">\(n\)</span> grows. Consistency does not require unbiasedness.</p>

<p>To examine properties of the sample mean as an estimator for the corresponding population mean, consider the following <tt>R</tt> example.</p>
<p>We generate a population <tt>pop</tt> consisting of observations <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(i=1,\dots,10000\)</span> that origin from a normal distribution with mean <span class="math inline">\(\mu = 10\)</span> and variance <span class="math inline">\(\sigma^2 = 1\)</span>.</p>
<p>To investigate the behavior of the estimator <span class="math inline">\(\hat{\mu} = \bar{Y}\)</span> we can draw random samples from this population and calculate <span class="math inline">\(\bar{Y}\)</span> for each of them. This is easily done by making use of the function <tt>replicate()</tt>. The argument <tt>expr</tt> is evaluated <tt>n</tt> times. In this case we draw samples of sizes <span class="math inline">\(n=5\)</span> and <span class="math inline">\(n=25\)</span>, compute the sample means and repeat this exactly <span class="math inline">\(N=25000\)</span> times.</p>
<p>For comparison purposes we store results for the estimator <span class="math inline">\(Y_1\)</span>, the first observation in a sample for a sample of size <span class="math inline">\(5\)</span>, separately.</p>
<pre class="r"><code># generate a fictious population
pop &lt;- rnorm(10000, 10, 1)
# sample from the population and estimate the mean
est1 &lt;- replicate(expr = mean(sample(x = pop, size = 5)), n = 25000)
est2 &lt;- replicate(expr = mean(sample(x = pop, size = 25)), n = 25000)
fo &lt;- replicate(expr = sample(x = pop, size = 5)[1], n = 25000)</code></pre>
<p>Check that <tt>est1</tt> and <tt>est2</tt> are vectors of length <span class="math inline">\(25000\)</span>:</p>
<pre class="r"><code># check if object type is vector
is.vector(est1)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>is.vector(est2)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code># check length
length(est1)</code></pre>
<pre><code>## [1] 25000</code></pre>
<pre class="r"><code>length(est2)</code></pre>
<pre><code>## [1] 25000</code></pre>
<p>The code chunk below produces a plot of the sampling distributions of the estimators <span class="math inline">\(\bar{Y}\)</span> and <span class="math inline">\(Y_1\)</span> on the basis of the <span class="math inline">\(25000\)</span> samples in each case. We also plot the density function of the <span class="math inline">\(\mathcal{N}(10,1)\)</span> distribution.</p>
<div class="unfolded">
<pre class="r"><code># plot density estimate Y_1
plot(density(fo), 
      col = &#39;green&#39;, 
      lwd = 2,
      ylim = c(0, 2),
      xlab = &#39;estimates&#39;,
      main = &#39;Sampling Distributions of Unbiased Estimators&#39;)
# add density estimate for the distribution of the sample mean with n=5 to the plot
lines(density(est1), 
     col = &#39;steelblue&#39;, 
     lwd = 2, 
     bty = &#39;l&#39;)
# add density estimate for the distribution of the sample mean with n=25 to the plot
lines(density(est2), 
      col = &#39;red2&#39;, 
      lwd = 2)
# add a vertical line at the true parameter
abline(v = 10, lty = 2)
# add N(10,1) density to the plot
curve(dnorm(x, mean = 10), 
     lwd = 2,
     lty = 2,
     add = T)
# add a legend
legend(&quot;topleft&quot;,
       legend = c(&quot;N(10,1)&quot;,
                  expression(Y[1]),
                  expression(bar(Y) ~ n == 5),
                  expression(bar(Y) ~ n == 25)
                  ), 
       lty = c(2, 1, 1, 1), 
       col = c(&#39;black&#39;,&#39;green&#39;, &#39;steelblue&#39;, &#39;red2&#39;),
       lwd = 2)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-11-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>First, <em>all</em> sampling distributions (represented by the solid lines) are centered around <span class="math inline">\(\mu = 10\)</span>. This is evidence for the <em>unbiasedness</em> of <span class="math inline">\(Y_1\)</span>, <span class="math inline">\(\overline{Y}_{5}\)</span> and <span class="math inline">\(\overline{Y}_{25}\)</span>. Of course, the theoretical density <span class="math inline">\(\mathcal{N}(10,1)\)</span> is centered at <span class="math inline">\(10\)</span>, too.</p>
<p>Next, have a look at the spread of the sampling distributions. Several things are noteworthy:</p>
<ul>
<li><p>The sampling distribution of <span class="math inline">\(Y_1\)</span> (green curve) tracks the density of the <span class="math inline">\(\mathcal{N}(10,1)\)</span> distribution (black dashed line) pretty closely. In fact, the sampling distribution of <span class="math inline">\(Y_1\)</span> is the <span class="math inline">\(\mathcal{N}(10,1)\)</span> distribution. This is less surprising if you keep in mind that the <span class="math inline">\(Y_1\)</span> estimator does nothing but reporting an observation that is randomly selected from a population with <span class="math inline">\(\mathcal{N}(10,1)\)</span> distribution. Hence, <span class="math inline">\(Y_1 \sim \mathcal{N}(10,1)\)</span>. Note that this result does not depend on the sample size <span class="math inline">\(n\)</span>: the sampling distribution of <span class="math inline">\(Y_1\)</span> <em>is always</em> the population distribution, no matter how large the sample is. <span class="math inline">\(Y_1\)</span> is a good a estimate of <span class="math inline">\(\mu_Y\)</span>, but we can do better.</p></li>
<li><p>Both sampling distributions of <span class="math inline">\(\overline{Y}\)</span> show less dispersion than the sampling distribution of <span class="math inline">\(Y_1\)</span>. This means that <span class="math inline">\(\overline{Y}\)</span> has a lower variance than <span class="math inline">\(Y_1\)</span>. In view of Key Concept 9, we find that <span class="math inline">\(\overline{Y}\)</span> is a more efficient estimator than <span class="math inline">\(Y_1\)</span>. In fact, this holds for all <span class="math inline">\(n&gt;1\)</span>.</p></li>
<li><p><span class="math inline">\(\overline{Y}\)</span> shows a behavior illustrating consistency (see Key Concept 9). The blue and the red densities are much more concentrated around <span class="math inline">\(\mu=10\)</span> than the green one. As the number of observations is increased from <span class="math inline">\(1\)</span> to <span class="math inline">\(5\)</span>, the sampling distribution tightens around the true parameter. Increasing the sample size to <span class="math inline">\(25\)</span>, this effect becomes more apparent. This implies that the probability of obtaining estimates that are close to the true value increases with <span class="math inline">\(n\)</span>.</p></li>
</ul>
<p>Try out different values for the sample size and see how the sampling distribution of <span class="math inline">\(\overline{Y}\)</span> changes!</p>
<div id="overliney-is-the-least-squares-estimator-of-mu_y" class="section level4 unnumbered">
<h4><span class="math inline">\(\overline{Y}\)</span> is the Least Squares Estimator of <span class="math inline">\(\mu_Y\)</span></h4>
<p>Assume you have some observations <span class="math inline">\(Y_1,\dots,Y_n\)</span> on <span class="math inline">\(Y \sim \mathcal{N}(10,1)\)</span> (which is unknown) and would like to find an estimator <span class="math inline">\(m\)</span> that predicts the observations as well as possible. By good we mean to choose <span class="math inline">\(m\)</span> such that the total squared deviation between the predicted value and the observed values is small. Mathematically, this means we want to find an <span class="math inline">\(m\)</span> that minimizes</p>
<p><span class="math display">\[\begin{equation}
  \sum_{i=1}^n (Y_i - m)^2. 
\end{equation}\]</span></p>
<p>Think of <span class="math inline">\(Y_i - m\)</span> as the mistake made when predicting <span class="math inline">\(Y_i\)</span> by <span class="math inline">\(m\)</span>. We could also minimize the sum of absolute deviations from <span class="math inline">\(m\)</span> but minimizing the sum of squared deviations is mathematically more convenient (and will lead to a different result). That is why the estimator we are looking for is called the <em>least squares estimator</em>. <span class="math inline">\(m = \overline{Y}\)</span>, the sample mean, is this estimator.</p>
<p>We can show this by generating a random sample and plotting the above equation as a function of <span class="math inline">\(m\)</span>.</p>
<pre class="r"><code># define the function and vectorize it
sqm &lt;- function(m) {
 sum((y-m)^2)
}
sqm &lt;- Vectorize(sqm)
# draw random sample and compute the mean
y &lt;- rnorm(100, 10, 1)
mean(y)</code></pre>
<pre><code>## [1] 10.1364</code></pre>
<div class="unfolded">
<pre class="r"><code># plot the objective function
curve(sqm(x), 
      from = -50, 
      to = 70,
      xlab = &quot;m&quot;,
      ylab = &quot;sqm(m)&quot;)
# add vertical line at mean(y)
abline(v = mean(y), 
       lty = 2, 
       col = &quot;darkred&quot;)
# add annotation at mean(y)
text(x = mean(y), 
     y = 0, 
     labels = paste(round(mean(y), 2)))</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-13-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>Notice that the equation above is a quadratic function so that there is only one minimum. The plot shows that this minimum lies exactly at the sample mean of the sample data.</p>
<ul>
<p>Some <tt>R</tt> functions can only interact with functions that take a vector as an input and evaluate the function body on every entry of the vector, for example <tt>curve()</tt>. We call such functions vectorized functions and it is often a good idea to write vectorized functions yourself, although this is cumbersome in some cases. Having a vectorized function in <tt>R</tt> is never a drawback since these functions work on both single values and vectors.<br />
  Let us look at the function <tt>sqm()</tt>, which is non-vectorized:</p>
<p><tt> sqm &lt;- function(m) {<br />
     sum((y-m)^2) #body of the function<br />
} </tt></p>
<p>Providing, e.g., <tt>c(1,2,3)</tt> as the argument <tt>m</tt> would cause an error since then the operation <tt>y-m</tt> is invalid: the vectors <tt>y</tt> and <tt>m</tt> are of incompatible dimensions. This is why we cannot use <tt>sqm()</tt> in conjunction with <tt>curve()</tt>.</p> 
<p>Here <tt>Vectorize()</tt> comes into play. It generates a vectorized version of a non-vectorized function.</p>
</ul>
</div>
<div id="why-random-sampling-is-important" class="section level4 unnumbered">
<h4>Why Random Sampling is Important</h4>
<p>So far, we assumed (sometimes implicitly) that the observed data <span class="math inline">\(Y_1, \dots, Y_n\)</span> are the result of a sampling process that satisfies the assumption of simple random sampling. This assumption often is fulfilled when estimating a population mean using <span class="math inline">\(\overline{Y}\)</span>. If this is not the case, estimates may be biased.</p>
<p>Let us fall back to <tt>pop</tt>, the fictive population of <span class="math inline">\(10000\)</span> observations and compute the population mean <span class="math inline">\(\mu_{\texttt{pop}}\)</span>:</p>
<pre class="r"><code># compute the population mean of pop
mean(pop)</code></pre>
<pre><code>## [1] 9.992604</code></pre>
<p>Next we sample <span class="math inline">\(10\)</span> observations from <tt>pop</tt> with <tt>sample()</tt> and estimate <span class="math inline">\(\mu_{\texttt{pop}}\)</span> using <span class="math inline">\(\overline{Y}\)</span> repeatedly. However, now we use a sampling scheme that deviates from simple random sampling: instead of ensuring that each member of the population has the same chance to end up in a sample, we assign a higher probability of being sampled to the <span class="math inline">\(2500\)</span> smallest observations of the population by setting the argument <tt>prop</tt> to a suitable vector of probability weights:</p>
<pre class="r"><code># simulate outcomes for the sample mean when the i.i.d. assumption fails
est3 &lt;-  replicate(n = 25000, 
                   expr = mean(sample(x = sort(pop), 
                                      size = 10, 
                                      prob = c(rep(4, 2500), rep(1, 7500)))))
# compute the sample mean of the outcomes
mean(est3)</code></pre>
<pre><code>## [1] 9.444113</code></pre>
<p>Next we plot the sampling distribution of <span class="math inline">\(\overline{Y}\)</span> for this non-i.i.d. case and compare it to the sampling distribution when the i.i.d. assumption holds.</p>
<div class="unfolded">
<pre class="r"><code># sampling distribution of sample mean, i.i.d. holds, n=25
plot(density(est2), 
      col = &#39;steelblue&#39;,
      lwd = 2,
      xlim = c(8, 11),
      xlab = &#39;Estimates&#39;,
      main = &#39;When the i.i.d. Assumption Fails&#39;)
# sampling distribution of sample mean, i.i.d. fails, n=25
lines(density(est3),
      col = &#39;red2&#39;,
      lwd = 2)
# add a legend
legend(&quot;topleft&quot;,
       legend = c(expression(bar(Y)[n == 25]~&quot;, i.i.d. fails&quot;),
                  expression(bar(Y)[n == 25]~&quot;, i.i.d. holds&quot;)
                  ), 
       lty = c(1, 1), 
       col = c(&#39;red2&#39;, &#39;steelblue&#39;),
       lwd = 2)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-16-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>Here, the failure of the i.i.d. assumption implies that, on average, we <em>underestimate</em> <span class="math inline">\(\mu_Y\)</span> using <span class="math inline">\(\overline{Y}\)</span>: the corresponding distribution of <span class="math inline">\(\overline{Y}\)</span> is shifted to the left. In other words, <span class="math inline">\(\overline{Y}\)</span> is a <em>biased</em> estimator for <span class="math inline">\(\mu_Y\)</span> if the i.i.d. assumption does not hold.</p>
</div>
</div>
<div id="hypothesis-tests-concerning-the-population-mean" class="section level2">
<h2><span class="header-section-number">1.3</span> Population Mean Hypothesis Test</h2>
<p>In this section we briefly review concepts in hypothesis testing and discuss how to conduct hypothesis tests in <tt>R</tt>. We focus on drawing inferences about an unknown population mean.</p>
<div id="about-hypotheses-and-hypothesis-testing" class="section level4 unnumbered">
<h4>Hypotheses and Hypothesis Testing</h4>
<p>In a significance test, we want to exploit the information contained in a sample as evidence in favor or against a hypothesis. Essentially, hypotheses are simple questions that can be answered by ‘yes’ or ‘no’. In a hypothesis test we typically deal with two different hypotheses:</p>
<ul>
<li><p>The <em>null hypothesis</em>, denoted <span class="math inline">\(H_0\)</span>, is the hypothesis we are interested in testing.</p></li>
<li><p>There must be an <em>alternative hypothesis</em>, denoted <span class="math inline">\(H_1\)</span>, the hypothesis that is thought to hold if the null hypothesis is rejected.</p></li>
</ul>
<p>The null hypothesis that the population mean of <span class="math inline">\(Y\)</span> equals the value <span class="math inline">\(\mu_{Y,0}\)</span> is written as</p>
<p><span class="math display">\[ H_0: E(Y) = \mu_{Y,0}. \]</span></p>
<p>Often the alternative hypothesis chosen is the most general one,</p>
<p><span class="math display">\[ H_1: E(Y) \neq \mu_{Y,0}, \]</span></p>
<p>meaning that <span class="math inline">\(E(Y)\)</span> may be anything but the value under the null hypothesis. This is called a <em>two-sided</em> alternative.</p>
<p>For the sake of brevity, we only consider two-sided alternatives in the subsequent sections of this chapter.</p>
</div>
<div id="the-p-value" class="section level3 unnumbered">
<h3>The p-Value</h3>
<p>Assume that the null hypothesis is <em>true</em>. The <span class="math inline">\(p\)</span>-value is the probability of drawing data and observing a corresponding test statistic that is at least as adverse to what is stated under the null hypothesis as the test statistic actually computed using the sample data.</p>
<p>In the context of the population mean and the sample mean, this definition can be stated mathematically in the following way:</p>
<p><span class="math display">\[\begin{equation}
p \text{-value} = P_{H_0}\left[ \lvert \overline{Y} - \mu_{Y,0} \rvert &gt; \lvert \overline{Y}^{act} - \mu_{Y,0} \rvert \right]
\end{equation}\]</span></p>
<p>In the equation above, <span class="math inline">\(\overline{Y}^{act}\)</span> is the mean of the sample actually computed. Consequently, in order to compute the <span class="math inline">\(p\)</span>-value, knowledge about the sampling distribution of <span class="math inline">\(\overline{Y}\)</span> when the null hypothesis is true is required. However in most cases the sampling distribution of <span class="math inline">\(\overline{Y}\)</span> is unknown. Fortunately, as stated by the CLT (see Key Concept 7), the large-sample approximation <span class="math display">\[ \overline{Y} \approx \mathcal{N}(\mu_{Y,0}, \, \sigma^2_{\overline{Y}}) \ \ , \ \ \sigma^2_{\overline{Y}} = \frac{\sigma_Y^2}{n} \]</span></p>
<p>can be made under the null. Thus,</p>
<p><span class="math display">\[ \frac{\overline{Y} - \mu_{Y,0}}{\sigma_Y/\sqrt{n}} \sim \mathcal{N}(0,1). \]</span></p>
<p>So in large samples, the <span class="math inline">\(p\)</span>-value can be computed <em>without</em> knowledge of the exact sampling distribution of <span class="math inline">\(\overline{Y}\)</span>.</p>
</div>
<div id="calculating-the-p-value-when-the-standard-deviation-is-known" class="section level3 unnumbered">
<h3>p-Values with Known SD</h3>
<p>For now, let us assume that <span class="math inline">\(\sigma_{\overline{Y}}\)</span> is known. Then, we can rewrite the equation above as</p>
<p><span class="math display">\[\begin{align}
p \text{-value} =&amp; \, P_{H_0}\left[ \left\lvert \frac{\overline{Y} - \mu_{Y,0}}{\sigma_{\overline{Y}}} \right\rvert &gt; \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_{\overline{Y}}} \right\rvert \right] \\
=&amp; \, 2 \cdot \Phi \left[ - \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_{\overline{Y}}}  \right\rvert\right]. 
\end{align}\]</span></p>
<p>The <span class="math inline">\(p\)</span>-value can be seen as the area in the tails of the <span class="math inline">\(\mathcal{N}(0,1)\)</span> distribution that lies beyond</p>
<p><span class="math display">\[\begin{equation}
\pm \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_{\overline{Y}}} \right\rvert 
\end{equation}\]</span></p>
<p>We now use <tt>R</tt> to visualize what is stated in the last two equations:</p>
<div class="unfolded">
<pre class="r"><code># plot the standard normal density on the interval [-4,4]
curve(dnorm(x),
      xlim = c(-4, 4),
      main = &#39;Calculating a p-Value&#39;,
      yaxs = &#39;i&#39;,
      xlab = &#39;z&#39;,
      ylab = &#39;&#39;,
      lwd = 2,
      axes = &#39;F&#39;)
# add x-axis
axis(1, 
     at = c(-1.5, 0, 1.5), 
     padj = 0.75,
     labels = c(expression(-frac(bar(Y)^&quot;act&quot;~-~bar(mu)[Y,0], sigma[bar(Y)])),
                0,
                expression(frac(bar(Y)^&quot;act&quot;~-~bar(mu)[Y,0], sigma[bar(Y)]))))
# shade p-value/2 region in left tail
polygon(x = c(-6, seq(-6, -1.5, 0.01), -1.5),
        y = c(0, dnorm(seq(-6, -1.5, 0.01)),0), 
        col = &#39;steelblue&#39;)
# shade p-value/2 region in right tail
polygon(x = c(1.5, seq(1.5, 6, 0.01), 6),
        y = c(0, dnorm(seq(1.5, 6, 0.01)), 0), 
        col = &#39;steelblue&#39;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-17-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="SVSSDASE" class="section level3 unnumbered">
<h3>Sample Variance</h3>
<p>If <span class="math inline">\(\sigma^2_Y\)</span> is unknown, it must be estimated. This can be done using the sample variance</p>
<p><span class="math display">\[\begin{equation}
s_Y^2 = \frac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y})^2.
\end{equation}\]</span></p>
<p>Furthermore</p>
<p><span class="math display">\[\begin{equation}
s_Y = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y})^2}
\end{equation}\]</span></p>
<p>is a suitable estimator for the standard deviation of <span class="math inline">\(Y\)</span>. In <tt>R</tt>, <span class="math inline">\(s_Y\)</span> is implemented in the function <tt>sd()</tt>, see <code>?sd</code>.</p>
<p>Using <tt>R</tt> we can illustrate that <span class="math inline">\(s_Y\)</span> is a consistent estimator for <span class="math inline">\(\sigma_Y\)</span> with</p>
<p><span class="math display">\[ s_Y \overset{p}{\longrightarrow} \sigma_Y. \]</span></p>
<p>The idea here is to generate a large number of samples <span class="math inline">\(Y_1,\dots,Y_n\)</span> where, <span class="math inline">\(Y\sim \mathcal{N}(10,10)\)</span> say, estimate <span class="math inline">\(\sigma_Y\)</span> using <span class="math inline">\(s_Y\)</span> and investigate how the distribution of <span class="math inline">\(s_Y\)</span> changes as <span class="math inline">\(n\)</span> gets larger.</p>
<div class="unfolded">
<pre class="r"><code># vector of sample sizes
n &lt;- c(10000, 5000, 2000, 1000, 500)
# sample observations, estimate using &#39;sd()&#39; and plot the estimated distributions
sq_y &lt;- replicate(n = 10000, expr = sd(rnorm(n[1], 10, 10)))
plot(density(sq_y),
     main = expression(&#39;Sampling Distributions of&#39; ~ s[Y]),
     xlab = expression(s[y]),
     lwd = 2)
for (i in 2:length(n)) {
  sq_y &lt;- replicate(n = 10000, expr = sd(rnorm(n[i], 10, 10)))
  lines(density(sq_y), 
        col = i, 
        lwd = 2)
}
# add a legend
legend(&quot;topleft&quot;,
       legend = c(expression(n == 10000),
                  expression(n == 5000),
                  expression(n == 2000),
                  expression(n == 1000),
                  expression(n == 500)), 
       col = 1:5,
       lwd = 2)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-18-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>The plot shows that the distribution of <span class="math inline">\(s_Y\)</span> tightens around the true value <span class="math inline">\(\sigma_Y = 10\)</span> as <span class="math inline">\(n\)</span> increases.</p>
<p>The function that estimates the standard deviation of an estimator is called the <em>standard error of the estimator</em>. Key Concept 10 summarizes the terminology in the context of the sample mean.</p>
<div id="KC10" class="keyconcept">
<h3 class="right">
Key Concept 10
</h3>
<h3 class="left">
The Standard Error of <span class="math inline">\(\overline{Y}\)</span>
</h3>
<p>Take an i.i.d. sample <span class="math inline">\(Y_1, \dots, Y_n\)</span>. The mean of <span class="math inline">\(Y\)</span> is consistently estimated by <span class="math inline">\(\overline{Y}\)</span>, the sample mean of the <span class="math inline">\(Y_i\)</span>. Since <span class="math inline">\(\overline{Y}\)</span> is a random variable, it has a sampling distribution with variance <span class="math inline">\(\frac{\sigma_Y^2}{n}\)</span>. The standard error of <span class="math inline">\(\overline{Y}\)</span>, denoted <span class="math inline">\(SE(\overline{Y})\)</span> is an estimator of the standard deviation of <span class="math inline">\(\overline{Y}\)</span>: <span class="math display">\[ SE(\overline{Y}) = \hat\sigma_{\overline{Y}} = \frac{s_Y}{\sqrt{n}} \]</span> The caret (^) over <span class="math inline">\(\sigma\)</span> indicates that <span class="math inline">\(\hat\sigma_{\overline{Y}}\)</span> is an estimator for <span class="math inline">\(\sigma_{\overline{Y}}\)</span>.</p>
</div>
<p>As an example to underpin Key Concept 10, consider a sample of <span class="math inline">\(n=100\)</span> i.i.d. observations of the Bernoulli distributed variable <span class="math inline">\(Y\)</span> with success probability <span class="math inline">\(p=0.1\)</span>. Thus <span class="math inline">\(E(Y)=p=0.1\)</span> and <span class="math inline">\(\text{Var}(Y)=p(1-p)\)</span>. <span class="math inline">\(E(Y)\)</span> can be estimated by <span class="math inline">\(\overline{Y}\)</span>, which then has variance</p>
<p><span class="math display">\[ \sigma^2_{\overline{Y}} = p(1-p)/n = 0.0009 \]</span></p>
<p>and standard deviation</p>
<p><span class="math display">\[ \sigma_{\overline{Y}} = \sqrt{p(1-p)/n} = 0.03. \]</span></p>
<p>In this case the standard error of <span class="math inline">\(\overline{Y}\)</span> can be estimated by</p>
<p><span class="math display">\[ SE(\overline{Y}) = \sqrt{\overline{Y}(1-\overline{Y})/n}. \]</span></p>
<p>Let us check whether <span class="math inline">\(\overline{Y}\)</span> and <span class="math inline">\(SE(\overline{Y})\)</span> estimate the respective true values, on average.</p>
<pre class="r"><code># draw 10000 samples of size 100 and estimate the mean of Y and
# estimate the standard error of the sample mean
mean_estimates &lt;- numeric(10000)
se_estimates &lt;- numeric(10000)
for (i in 1:10000) {
  
  s &lt;- sample(0:1, 
              size = 100,  
              prob = c(0.9, 0.1),
              replace = T)
  
  mean_estimates[i] &lt;- mean(s)
  se_estimates[i] &lt;- sqrt(mean(s) * (1 - mean(s)) / 100)
}
mean(mean_estimates)</code></pre>
<pre><code>## [1] 0.10047</code></pre>
<pre class="r"><code>mean(se_estimates)</code></pre>
<pre><code>## [1] 0.02961587</code></pre>
<p>Both estimators seem to be unbiased for the true parameters. In fact, this is true for the sample mean, but not for <span class="math inline">\(SE(\overline{Y})\)</span>. However, both estimators are <em>consistent</em> for the true parameters.</p>
</div>
<div id="calculating-the-p-value-when-the-standard-deviation-is-unknown" class="section level3 unnumbered">
<h3>p-value with Unknown SD</h3>
<p>When <span class="math inline">\(\sigma_Y\)</span> is unknown, the <span class="math inline">\(p\)</span>-value for a hypothesis test concerning <span class="math inline">\(\mu_Y\)</span> using <span class="math inline">\(\overline{Y}\)</span> can be computed by replacing <span class="math inline">\(\sigma_{\overline{Y}}\)</span> by the standard error <span class="math inline">\(SE(\overline{Y}) = \hat\sigma_{\overline{Y}}\)</span>. Then,</p>
<p><span class="math display">\[ p\text{-value} = 2\cdot\Phi\left(-\left\lvert \frac{\overline{Y}^{act}-\mu_{Y,0}}{SE(\overline{Y})} \right\rvert \right). \]</span></p>
<p>This is easily done in <tt>R</tt>:</p>
<pre class="r"><code># sample and estimate, compute standard error
samplemean_act &lt;- mean(
  sample(0:1, 
         prob = c(0.9, 0.1), 
         replace = T, 
         size = 100))
SE_samplemean &lt;- sqrt(samplemean_act * (1 - samplemean_act) / 100)
# null hypothesis
mean_h0 &lt;- 0.1
# compute the p-value
pvalue &lt;- 2 * pnorm(- abs(samplemean_act - mean_h0) / SE_samplemean)
pvalue</code></pre>
<pre><code>## [1] 0.7492705</code></pre>
<p>Later, we will encounter more convenient approaches to obtain <span class="math inline">\(t\)</span>-statistics and <span class="math inline">\(p\)</span>-values using <tt>R</tt>.</p>
</div>
<div id="the-t-statistic" class="section level3 unnumbered">
<h3>The t-statistic</h3>
<p>In hypothesis testing, the standardized sample average</p>
<p><span class="math display">\[\begin{equation}
t = \frac{\overline{Y} - \mu_{Y,0}}{SE(\overline{Y})} 
\end{equation}\]</span></p>
<p>is called a <span class="math inline">\(t\)</span>-statistic. This <span class="math inline">\(t\)</span>-statistic plays an important role in testing hypotheses about <span class="math inline">\(\mu_Y\)</span>. It is a prominent example of a test statistic.</p>
<p>Implicitly, we already have computed a <span class="math inline">\(t\)</span>-statistic for <span class="math inline">\(\overline{Y}\)</span> in the previous code chunk.</p>
<pre class="r"><code># compute a t-statistic for the sample mean
tstatistic &lt;- (samplemean_act - mean_h0) / SE_samplemean
tstatistic</code></pre>
<pre><code>## [1] 0.3196014</code></pre>
<p>Using <tt>R</tt> we can illustrate that if <span class="math inline">\(\mu_{Y,0}\)</span> equals the true value, that is, if the null hypothesis is true, the <span class="math inline">\(t\)</span>-statistic equation is approximately <span class="math inline">\(\mathcal{N}(0,1)\)</span> distributed when <span class="math inline">\(n\)</span> is large.</p>
<pre class="r"><code># prepare empty vector for t-statistics
tstatistics &lt;- numeric(10000)
# set sample size
n &lt;- 300
# simulate 10000 t-statistics
for (i in 1:10000) {
  
  s &lt;- sample(0:1, 
              size = n,  
              prob = c(0.9, 0.1),
              replace = T)
  
  tstatistics[i] &lt;- (mean(s)-0.1)/sqrt(var(s)/n)
  
}</code></pre>
<div class="unfolded">
<p>In the simulation above, we estimate the variance of the <span class="math inline">\(Y_i\)</span> using <tt>var(s)</tt>. This is more general then <tt>mean(s)*(1-mean(s))</tt> since the latter requires that the data are Bernoulli distributed and that we know this.</p>
<pre class="r"><code># plot density and compare to N(0,1) density
plot(density(tstatistics),
     xlab = &#39;t-statistic&#39;,
     main = &#39;Estimated Distribution of the t-statistic when n=300&#39;,
     lwd = 2,
     xlim = c(-4, 4),
     col = &#39;steelblue&#39;)
# N(0,1) density (dashed)
curve(dnorm(x), 
      add = T, 
      lty = 2, 
      lwd = 2)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-25-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>Judging from the plot, the normal approximation works reasonably well for the chosen sample size. This normal approximation has already been used in the definition of the <span class="math inline">\(p\)</span>-value, above.</p>
</div>
<div id="hypothesis-testing-with-a-prespecified-significance-level" class="section level3 unnumbered">
<h3>Hypothesis Testing</h3>
<div id="KC11" class="keyconcept">
<h3 class="right">
Key Concept 11
</h3>
<h3 class="left">
Hypothesis Testing Terminology
</h3>
<p>In hypothesis testing, two types of mistakes are possible: 1. The null hypothesis <em>is</em> rejected although it is true (type-I-error)<br />
2. The null hypothesis <em>is not</em> rejected although it is false (type-II-error) The <strong>significance level</strong> of the test is the probability to commit a type-I-error we are willing to accept in advance. E.g., using a prespecified significance level of <span class="math inline">\(0.05\)</span>, we reject the null hypothesis if and only if the <span class="math inline">\(p\)</span>-value is less than <span class="math inline">\(0.05\)</span>. The significance level is chosen before the test is conducted. An equivalent procedure is to reject the null hypothesis if the observed test statistic is, in absolute value terms, larger than the <strong>critical value</strong> of the test statistic. The critical value is determined by the significance level chosen and defines two disjoint sets of values which are called <strong>acceptance region</strong> and <strong>rejection region</strong>. The acceptance region contains all values of the test statistic for which the test does not reject while the rejection region contains all the values for which the test does reject. The <strong><span class="math inline">\(p\)</span>-value</strong> is the probability that, in repeated sampling under the same conditions a test statistic is observed that provides just as much evidence against the null hypothesis as the test statistic actually observed. The actual probability that the test rejects the true null hypothesis is called the <strong>size of the test</strong>. In an ideal setting, the size does equals the significance level. The probability that the test correctly rejects a false null hypothesis is called <strong>power</strong>.</p>
</div>
<p>Reconsider the <tt>pvalue</tt> computed further above:</p>
<pre class="r"><code># check whether p-value &lt; 0.05
pvalue &lt; 0.05</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>The condition is not fulfilled so we do not reject the null hypothesis correctly.</p>
<p>When working with a <span class="math inline">\(t\)</span>-statistic instead, it is equivalent to apply the following rule:</p>
<p><span class="math display">\[ \text{Reject } H_0 \text{ if } \lvert t^{act} \rvert &gt; 1.96 \]</span></p>
<p>We reject the null hypothesis at the significance level of <span class="math inline">\(5\%\)</span> if the computed <span class="math inline">\(t\)</span>-statistic lies beyond the critical value of 1.96 in absolute value terms. <span class="math inline">\(1.96\)</span> is the <span class="math inline">\(0.975\)</span>-quantile of the standard normal distribution.</p>
<pre class="r"><code># check the critical value
qnorm(p = 0.975)</code></pre>
<pre><code>## [1] 1.959964</code></pre>
<pre class="r"><code># check whether the null is rejected using the t-statistic computed further above
abs(tstatistic) &gt; 1.96</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>Just like using the <span class="math inline">\(p\)</span>-value, we cannot reject the null hypothesis using the corresponding <span class="math inline">\(t\)</span>-statistic. Key Concept 12 summarizes the procedure of performing a two-sided hypothesis test about the population mean <span class="math inline">\(E(Y)\)</span>.</p>
<div id="KC12" class="keyconcept">
<h3 class="right">
Key Concept 12
</h3>
<h3 class="left">
Testing Against the Alternative</span>
</h3>
<ol style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(\mu_{Y}\)</span> using <span class="math inline">\(\overline{Y}\)</span> and compute the <span class="math inline">\(SE(\overline{Y})\)</span>, standard error of <span class="math inline">\(\overline{Y}\)</span>.</li>
<li>Compute the <span class="math inline">\(t\)</span>-statistic.</li>
<li>Compute the <span class="math inline">\(p\)</span>-value and reject the null hypothesis at the <span class="math inline">\(5\%\)</span> level of significance if the <span class="math inline">\(p\)</span>-value is smaller than <span class="math inline">\(0.05\)</span> or, equivalently, if <span class="math display">\[ \left\lvert t^{act} \right\rvert &gt; 1.96. \]</span></li>
</ol>
</div>
</div>
<div id="one-sided-alternatives" class="section level3 unnumbered">
<h3>One-sided Alternatives</h3>
<p>Sometimes we are interested in testing if the mean is bigger or smaller than some value hypothesized under the null. If we anticipate that such a differential exists, a relevant alternative (to the null hypothesis that there is differential) is that the observed values for a group, <span class="math inline">\(\mu_Y\)</span> is <em>bigger</em> than <span class="math inline">\(\mu_{Y,0}\)</span>, the average for another group, which we assume to be known here for simplicity.</p>
<p>This is an example of a <em>right-sided test</em> and the hypotheses pair is chosen to be</p>
<p><span class="math display">\[ H_0: \mu_Y = \mu_{Y,0} \ \ \text{vs} \ \ H_1: \mu_Y &gt; \mu_{Y,0}. \]</span></p>
<p>We reject the null hypothesis if the computed test-statistic is larger than the critical value <span class="math inline">\(1.64\)</span>, the <span class="math inline">\(0.95\)</span>-quantile of the <span class="math inline">\(\mathcal{N}(0,1)\)</span> distribution. This ensures that <span class="math inline">\(1-0.95=5\%\)</span> probability mass remains in the area to the right of the critical value. As before, we can visualize this in <tt>R</tt> using the function <tt>polygon()</tt>.</p>
<div class="unfolded">
<pre class="r"><code># plot the standard normal density on the domain [-4,4]
curve(dnorm(x),
      xlim = c(-4, 4),
      main = &#39;Rejection Region of a Right-Sided Test&#39;,
      yaxs = &#39;i&#39;,
      xlab = &#39;t-statistic&#39;,
      ylab = &#39;&#39;,
      lwd = 2,
      axes = &#39;F&#39;)
# add the x-axis
axis(1, 
     at = c(-4, 0, 1.64, 4), 
     padj = 0.5,
     labels = c(&#39;&#39;, 0, expression(Phi^-1~(.95)==1.64), &#39;&#39;))
# shade the rejection region in the left tail
polygon(x = c(1.64, seq(1.64, 4, 0.01), 4),
        y = c(0, dnorm(seq(1.64, 4, 0.01)), 0), 
        col = &#39;darkred&#39;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-32-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>Analogously, for the left-sided test we have <span class="math display">\[H_0: \mu_Y = \mu_{Y,0} \ \ \text{vs.} \ \ H_1: \mu_Y &lt; \mu_{Y,0}.\]</span> The null is rejected if the observed test statistic falls short of the critical value which, for a test at the <span class="math inline">\(0.05\)</span> level of significance, is given by <span class="math inline">\(-1.64\)</span>, the <span class="math inline">\(0.05\)</span>-quantile of the <span class="math inline">\(\mathcal{N}(0,1)\)</span> distribution. <span class="math inline">\(5\%\)</span> probability mass lies to the left of the critical value.</p>
<p>It is straightforward to adapt the code chunk above to the case of a left-sided test. We only have to adjust the color shading and the tick marks.</p>
<div class="unfolded">
<pre class="r"><code># plot the the standard normal density on the domain [-4,4]
curve(dnorm(x),
      xlim = c(-4, 4),
      main = &#39;Rejection Region of a Left-Sided Test&#39;,
      yaxs = &#39;i&#39;,
      xlab = &#39;t-statistic&#39;,
      ylab = &#39;&#39;,
      lwd = 2,
      axes = &#39;F&#39;)
# add x-axis
axis(1, 
     at = c(-4, 0, -1.64, 4), 
     padj = 0.5,
     labels = c(&#39;&#39;, 0, expression(Phi^-1~(.05)==-1.64), &#39;&#39;))
# shade rejection region in right tail
polygon(x = c(-4, seq(-4, -1.64, 0.01), -1.64),
        y = c(0, dnorm(seq(-4, -1.64, 0.01)), 0), 
        col = &#39;darkred&#39;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-33-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="confidence-intervals-for-the-population-mean" class="section level2">
<h2><span class="header-section-number">1.4</span> Population Mean Confidence Interval</h2>
<p>As stressed before, we will never estimate the <em>exact</em> value of the population mean of <span class="math inline">\(Y\)</span> using a random sample. However, we can compute confidence intervals for the population mean. In general, a confidence interval for an unknown parameter is a recipe that, in repeated samples, yields intervals that contain the true parameter with a prespecified probability, the <em>confidence level</em>. Confidence intervals are computed using the information available in the sample. Since this information is the result of a random process, confidence intervals are random variables themselves.</p>
<p>Key Concept 13 shows how to compute confidence intervals for the unknown population mean <span class="math inline">\(E(Y)\)</span>.</p>
<div id="KC13" class="keyconcept">
<h3 class="right">
Key Concept 13
</h3>
<h3 class="left">
Mean Confidence Interval
</h3>
<p>A <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(\mu_Y\)</span> is a random variable that contains the true <span class="math inline">\(\mu_Y\)</span> in <span class="math inline">\(95\%\)</span> of all possible random samples. When <span class="math inline">\(n\)</span> is large we can use the normal approximation. Then, <span class="math inline">\(99\%\)</span>, <span class="math inline">\(95\%\)</span>, <span class="math inline">\(90\%\)</span> confidence intervals are <span class="math display">\[\begin{align}
&amp;99\%\text{ confidence interval for } \mu_Y = \left[ \overline{Y} \pm 2.58 \times SE(\overline{Y}) \right], \\
&amp;95\%\text{ confidence interval for } \mu_Y = \left[\overline{Y} \pm 1.96 \times SE(\overline{Y}) \right], \\
&amp;90\%\text{ confidence interval for } \mu_Y = \left[ \overline{Y} \pm 1.64 \times SE(\overline{Y}) \right].
\end{align}\]</span> These confidence intervals are sets of null hypotheses we cannot reject in a two-sided hypothesis test at the given level of confidence. Now consider the following statements. 1. In repeated sampling, the interval <span class="math display">\[ \left[ \overline{Y} \pm 1.96 \times SE(\overline{Y}) \right] \]</span> covers the true value of <span class="math inline">\(\mu_Y\)</span> with a probability of <span class="math inline">\(95\%\)</span>. 2. We have computed <span class="math inline">\(\overline{Y} = 5.1\)</span> and <span class="math inline">\(SE(\overline{Y})=2.5\)</span> so the interval <span class="math display">\[ \left[ 5.1  \pm 1.96 \times 2.5 \right] = \left[0.2,10\right] \]</span> covers the true value of <span class="math inline">\(\mu_Y\)</span> with a probability of <span class="math inline">\(95\%\)</span>. While 1. is right (this is in line with the definition above), 2. is wrong and I would never want to read such a sentence. The difference is that, while 1. is the definition of a random variable, 2. is one possible <em>outcome</em> of this random variable so there is no meaning in making any probabilistic statement about it. Either the computed interval does cover <span class="math inline">\(\mu_Y\)</span> <em>or</em> it does not!</p>
</div>
<p>In <tt>R</tt>, testing of hypotheses about the mean of a population on the basis of a random sample is very easy due to functions like <tt>t.test()</tt> from the <tt>stats</tt> package. It produces an object of type <tt>list</tt>. Luckily, one of the most simple ways to use <tt>t.test()</tt> is when you want to obtain a <span class="math inline">\(95\%\)</span> confidence interval for some population mean. We start by generating some random data and calling <tt>t.test()</tt> in conjunction with <tt>ls()</tt> to obtain a breakdown of the output components.</p>
<pre class="r"><code># set seed
set.seed(1)
# generate some sample data
sampledata &lt;- rnorm(100, 10, 10)
# check the type of the outcome produced by t.test
typeof(t.test(sampledata))</code></pre>
<pre><code>## [1] &quot;list&quot;</code></pre>
<pre class="r"><code># display the list elements produced by t.test
ls(t.test(sampledata))</code></pre>
<pre><code>##  [1] &quot;alternative&quot; &quot;conf.int&quot;    &quot;data.name&quot;   &quot;estimate&quot;    &quot;method&quot;     
##  [6] &quot;null.value&quot;  &quot;p.value&quot;     &quot;parameter&quot;   &quot;statistic&quot;   &quot;stderr&quot;</code></pre>
<p>Though we find that many items are reported, at the moment we are only interested in computing a <span class="math inline">\(95\%\)</span> confidence set for the mean.</p>
<pre class="r"><code>t.test(sampledata)$&quot;conf.int&quot;</code></pre>
<pre><code>## [1]  9.306651 12.871096
## attr(,&quot;conf.level&quot;)
## [1] 0.95</code></pre>
<p>This tells us that the <span class="math inline">\(95\%\)</span> confidence interval is</p>
<p><span class="math display">\[ \left[9.31, 12.87\right]. \]</span></p>
<p>In this example, the computed interval obviously does cover the true <span class="math inline">\(\mu_Y\)</span> which we know to be <span class="math inline">\(10\)</span>.</p>
<p>Let us have a look at the whole standard output produced by <tt>t.test()</tt>.</p>
<pre class="r"><code>t.test(sampledata)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  sampledata
## t = 12.346, df = 99, p-value &lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##   9.306651 12.871096
## sample estimates:
## mean of x 
##  11.08887</code></pre>
<p>We see that <tt>t.test()</tt> does not only compute a <span class="math inline">\(95\%\)</span> confidence interval but automatically conducts a two-sided significance test of the hypothesis <span class="math inline">\(H_0: \mu_Y = 0\)</span> at the level of <span class="math inline">\(5\%\)</span> and reports relevant parameters thereof: the alternative hypothesis, the estimated mean, the resulting <span class="math inline">\(t\)</span>-statistic, the degrees of freedom of the underlying <span class="math inline">\(t\)</span> distribution (<tt>t.test()</tt> does use perform the normal approximation) and the corresponding <span class="math inline">\(p\)</span>-value. This is very convenient!</p>
<p>In this example, we come to the conclusion that the population mean <em>is</em> significantly different from <span class="math inline">\(0\)</span> (which is correct) at the level of <span class="math inline">\(5\%\)</span>, since <span class="math inline">\(\mu_Y = 0\)</span> is not an element of the <span class="math inline">\(95\%\)</span> confidence interval</p>
<p><span class="math display">\[ 0 \in \left[9.31,12.87\right]. \]</span> We come to an equivalent result when using the <span class="math inline">\(p\)</span>-value rejection rule since</p>
<p><span class="math display">\[ p\text{-value} = 2.2\cdot 10^{-16} \ll 0.05. \]</span></p>
</div>
<div id="cmfdp" class="section level2">
<h2><span class="header-section-number">1.5</span> Comparing Population Means</h2>
<p>Suppose you are interested in the means of two different populations, denote them <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span>. More specifically, you are interested whether these population means are different from each other and plan to use a hypothesis test to verify this on the basis of independent sample data from both populations. A suitable pair of hypotheses is</p>
<p><span class="math display">\[\begin{equation}
H_0: \mu_1 - \mu_2 = d_0 \ \ \text{vs.} \ \ H_1: \mu_1 - \mu_2 \neq d_0 
\end{equation}\]</span></p>
<p>where <span class="math inline">\(d_0\)</span> denotes the hypothesized difference in means (so <span class="math inline">\(d_0=0\)</span> when the means are equal, under the null hypothesis). <span class="math inline">\(H_0\)</span> can be tested with the <span class="math inline">\(t\)</span>-statistic</p>
<p><span class="math display">\[\begin{equation}
t=\frac{(\overline{Y}_1 - \overline{Y}_2) - d_0}{SE(\overline{Y}_1 - \overline{Y}_2)} 
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{equation}
SE(\overline{Y}_1 - \overline{Y}_2) = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}.
\end{equation}\]</span></p>
<p>This is called a two sample <span class="math inline">\(t\)</span>-test. For large <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>, this is standard normal under the null hypothesis. Analogously to the simple <span class="math inline">\(t\)</span>-test we can compute confidence intervals for the true difference in population means:</p>
<p><span class="math display">\[ (\overline{Y}_1 - \overline{Y}_2) \pm 1.96 \times SE(\overline{Y}_1 - \overline{Y}_2) \]</span></p>
<p>is a <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(d\)</span>. <br> In <tt>R</tt>, hypotheses can be tested with <tt>t.test()</tt>, too. Note that <tt>t.test()</tt> chooses <span class="math inline">\(d_0 = 0\)</span> by default. This can be changed by setting the argument <tt>mu</tt> accordingly.</p>
<p>The subsequent code chunk demonstrates how to perform a two sample <span class="math inline">\(t\)</span>-test in <tt>R</tt> using simulated data.</p>
<pre class="r"><code># set random seed
set.seed(1)
# draw data from two different populations with equal mean
sample_pop1 &lt;- rnorm(100, 10, 10)
sample_pop2 &lt;- rnorm(100, 10, 20)
# perform a two sample t-test
t.test(sample_pop1, sample_pop2)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  sample_pop1 and sample_pop2
## t = 0.872, df = 140.52, p-value = 0.3847
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -2.338012  6.028083
## sample estimates:
## mean of x mean of y 
## 11.088874  9.243838</code></pre>
<p>We find that the two sample <span class="math inline">\(t\)</span>-test does not reject the (true) null hypothesis that <span class="math inline">\(d_0 = 0\)</span>.</p>
</div>
<div id="scatterplots-sample-covariance-and-sample-correlation" class="section level2">
<h2><span class="header-section-number">1.6</span> Scatterplots and Sample Covariance</h2>
<p>A scatter plot represents two dimensional data, for example <span class="math inline">\(n\)</span> observation on <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span>, by points in a coordinate system. It is very easy to generate scatter plots using the <tt>plot()</tt> function in <tt>R</tt>. Let us generate some artificial data and plot it.</p>
<pre class="r"><code># set random seed
set.seed(123)
# generate dataset
X &lt;- runif(n = 100, 
           min = 18, 
           max = 70)
Y &lt;- X + rnorm(n=100, 50, 15)
# plot observations
plot(X, 
     Y, 
     type = &quot;p&quot;,
     main = &quot;A Scatterplot of X and Y&quot;,
     xlab = &quot;X&quot;,
     ylab = &quot;Y&quot;,
     col = &quot;steelblue&quot;,
     pch = 19)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-40-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>The plot shows positive correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<div id="sample-covariance-and-correlation" class="section level4 unnumbered">
<h4>Sample Covariance and Correlation</h4>
<p>By now you should be familiar with the concepts of variance and covariance. Just like the variance, covariance and correlation of two variables are properties that relate to the (unknown) joint probability distribution of these variables. We can estimate covariance and correlation by means of suitable estimators using a sample <span class="math inline">\((X_i,Y_i)\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>.</p>
<p>The sample covariance</p>
<p><span class="math display">\[ s_{XY} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y}) \]</span></p>
<p>is an estimator for the population variance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> whereas the sample correlation</p>
<p><span class="math display">\[ r_{XY} = \frac{s_{XY}}{s_Xs_Y} \]</span> can be used to estimate the population correlation, a standardized measure for the strength of the linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>As for variance and standard deviation, these estimators are implemented as <tt>R</tt> functions in the <tt>stats</tt> package. We can use them to estimate population covariance and population correlation of the artificial data on age and earnings.</p>
<pre class="r"><code># compute sample covariance of X and Y
cov(X, Y)</code></pre>
<pre><code>## [1] 213.934</code></pre>
<pre class="r"><code># compute sample correlation between X and Y
cor(X, Y)</code></pre>
<pre><code>## [1] 0.706372</code></pre>
<pre class="r"><code># an equivalent way to compute the sample correlation
cov(X, Y) / (sd(X) * sd(Y))</code></pre>
<pre><code>## [1] 0.706372</code></pre>
<p>The estimates indicate that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are moderately correlated.</p>
<p>The next code chunk uses the function <tt>mvnorm()</tt> from package <tt>MASS</tt> <span class="citation">[@R-MASS]</span> to generate bivariate sample data with different degrees of correlation.</p>
<pre class="r"><code>library(MASS)
# set random seed
set.seed(1)
# positive correlation (0.81)
example1 &lt;- mvrnorm(100,
                    mu = c(0, 0), 
                    Sigma = matrix(c(2, 2, 2, 3), ncol = 2),
                    empirical = TRUE)
# negative correlation (-0.81)
example2 &lt;- mvrnorm(100,
                    mu = c(0, 0), 
                    Sigma = matrix(c(2, -2, -2, 3), ncol = 2),
                    empirical = TRUE)
# no correlation 
example3 &lt;- mvrnorm(100,
                    mu = c(0, 0), 
                    Sigma = matrix(c(1, 0, 0, 1), ncol = 2),
                    empirical = TRUE)
# no correlation (quadratic relationship)
X &lt;- seq(-3, 3, 0.01)
Y &lt;- - X^2 + rnorm(length(X))
example4 &lt;- cbind(X, Y)
# divide plot area as 2-by-2 array
par(mfrow = c(2, 2))
# plot datasets
plot(example1, col = &#39;steelblue&#39;, pch = 20, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, 
     main = &quot;Correlation = 0.81&quot;)
plot(example2, col = &#39;steelblue&#39;, pch = 20, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, 
     main = &quot;Correlation = -0.81&quot;)
plot(example3, col = &#39;steelblue&#39;, pch = 20, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, 
     main = &quot;Correlation = 0&quot;)
plot(example4, col = &#39;steelblue&#39;, pch = 20, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, 
     main = &quot;Correlation = 0&quot;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-42-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">1.7</span> Simple Linear Regression</h2>
<p>To start with an easy example, consider the following combinations of average patient satisfaction score and the average patient-doctor ratio in some fictional hospitals.</p>

<p>To work with these data in <tt>R</tt> we begin by generating two vectors: one for the patient-doctor ratios (<tt>PDR</tt>) and one for the patient scores (<tt>Score</tt>), both containing the data from the table above.</p>
<pre class="r"><code># Create sample data
PDR &lt;- c(15, 17, 19, 20, 22, 23.5, 25)
Score &lt;- c(680, 640, 670, 660, 630, 660, 635) 
# Print out sample data
PDR</code></pre>
<pre><code>## [1] 15.0 17.0 19.0 20.0 22.0 23.5 25.0</code></pre>
<pre class="r"><code>Score</code></pre>
<pre><code>## [1] 680 640 670 660 630 660 635</code></pre>
<p>To build a simple linear regression model, we hypothesise that the relationship between dependent and independent variables is linear, formally <span class="math display">\[ Y = b \cdot X + a. \]</span> For now, let us suppose that the function which relates patient score and patient-doctor ratio to each other is <span class="math display">\[Score = 713 - 3 \times PDR.\]</span></p>
<p>It is always a good idea to visualize the data you work with. Here, it is suitable to use <tt>plot()</tt> to produce a scatterplot with <tt>PDR</tt> on the <span class="math inline">\(x\)</span>-axis and <tt>Score</tt> on the <span class="math inline">\(y\)</span>-axis. Just call <code>plot(y_variable ~ x_variable)</code> whereby <tt>y_variable</tt> and <tt>x_variable</tt> are placeholders for the vectors of observations we want to plot. Furthermore, we might want to add a systematic relationship to the plot. To draw a straight line, <tt>R</tt> provides the function <tt>abline()</tt>. We just have to call this function with arguments <tt>a</tt> (representing the intercept) and <tt>b</tt> (representing the slope) after executing <tt>plot()</tt> in order to add the line to our plot.</p>
<div class="unfolded">
<pre class="r"><code># create a scatterplot of the data
plot(Score ~ PDR)
# add the systematic relationship to the plot
abline(a = 713, b = -3)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-45-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>We find that the line does not touch any of the points although we claimed that it represents the systematic relationship. The reason for this is randomness. Most of the time there are additional influences which imply that there is no bivariate relationship between the two variables.</p>
<p>In order to account for these differences between observed data and the systematic relationship, we extend our model from above by an <em>error term</em> <span class="math inline">\(u\)</span> which captures additional random effects. Put differently, <span class="math inline">\(u\)</span> accounts for all the differences between the regression line and the actual observed data. Beside pure randomness, these deviations could also arise from measurement errors or, as will be discussed later, could be the consequence of leaving out other factors that are relevant in explaining the dependent variable.</p>
<p>Which other factors are plausible in our example? For one thing, the patient scores might be driven by the doctors’ quality and/or the health of the patients prior to entering the hospital. It is also possible that in some hospitals, the patients were lucky and thus gave higher scores. For now, we will summarize such influences by an additive component:</p>
<p><span class="math display">\[ Score = \beta_0 + \beta_1 \times PDR + \text{other factors} \]</span></p>
<p>Of course this idea is very general as it can be easily extended to other situations that can be described with a linear model. The basic linear regression model we will work with hence is</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + u_i. \]</span></p>
<p>Key Concept 14 summarizes the terminology of the simple linear regression model.</p>
<div id="KC14" class="keyconcept">
<h3 class="right">
Key Concept 14
</h3>
<h3 class="left">
Linear Regression Model
</h3>
<p>
The linear regression model is <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + u_i\]</span> where - the index <span class="math inline">\(i\)</span> runs over the observations, <span class="math inline">\(i=1,\dots,n\)</span> - <span class="math inline">\(Y_i\)</span> is the <em>dependent variable</em>, the <em>regressand</em>, or simply the <em>left-hand variable</em> - <span class="math inline">\(X_i\)</span> is the <em>independent variable</em>, the <em>regressor</em>, or simply the <em>right-hand variable</em> - <span class="math inline">\(Y = \beta_0 + \beta_1 X\)</span> is the <em>population regression line</em> also called the <em>population regression function</em> - <span class="math inline">\(\beta_0\)</span> is the <em>intercept</em> of the population regression line - <span class="math inline">\(\beta_1\)</span> is the <em>slope</em> of the population regression line - <span class="math inline">\(u_i\)</span> is the <em>error term</em>.
</p>
</div>
</div>
<div id="estimating-the-coefficients-of-the-linear-regression-model" class="section level2">
<h2><span class="header-section-number">1.8</span> Estimating the Coefficients</h2>
<p>In practice, the intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span> of the population regression line are unknown. Therefore, we must employ data to estimate both unknown parameters. In the following, we will use a dataset to demonstrate how this is achieved. We want to relate patient scores to patient-doctor ratios measured in regional hospitals. The patient score is the average of their health and satisfaction scores. Again, we use the number of patients divided by the number of doctors (the patient-doctor ratio). The data set can be loaded using the function <tt>read.table()</tt> and can be downloaded from the course Moodle page.</p>
<pre class="r"><code>## # load the the data set in the workspace
## data &lt;- read.table(&quot;hospitals.txt&quot;,header=TRUE)</code></pre>
<p>With help of <tt>head()</tt> we get a first overview of our data. This function shows only the first 6 rows of the data set which prevents an overcrowded console output.</p>

<div class="rmdnote">
Press <tt>ctrl + L</tt> to clear the console. This command deletes any code that has been typed in and executed by you or printed to the console by <tt>R</tt> functions. The good news is that anything else is left untouched. You neither loose defined variables etc. nor the code history. It is still possible to recall previously executed <tt>R</tt> commands using the up and down keys. If you are working in <em>RStudio</em>, press <tt>ctrl + Up</tt> on your keyboard (<tt>CMD + Up</tt> on a Mac) to review a list of previously entered commands.
</div>

<pre class="r"><code>head(data)</code></pre>
<pre><code>##                                                                             
## 1 function (..., list = character(), package = NULL, lib.loc = NULL,        
## 2     verbose = getOption(&quot;verbose&quot;), envir = .GlobalEnv, overwrite = TRUE) 
## 3 {                                                                         
## 4     fileExt &lt;- function(x) {                                              
## 5         db &lt;- grepl(&quot;\\\\.[^.]+\\\\.(gz|bz2|xz)$&quot;, x)                     
## 6         ans &lt;- sub(&quot;.*\\\\.&quot;, &quot;&quot;, x)</code></pre>
<p>We find that the data set consists of plenty of variables and that most of them are numeric.</p>
<p>The two variables we are interested in (i.e., average patient score and the patient-doctor ratio) are <em>not</em> included. However, it is possible to calculate both from the provided data. To obtain the patient-doctor ratio, we simply divide the number of patients by the number of doctors (average on call number). The average patient score is the arithmetic mean of the score for satisfaction and the score of patient health after their hospital discharge. The next code chunk shows how the two variables can be constructed as vectors and how they are appended to <tt>data</tt>.</p>
<pre class="r"><code>data &lt;- read.table(&quot;hospitals.txt&quot;, header=TRUE)
# compute STR and append it to CASchools
data$PDR &lt;- data$patients/data$doctors 
# compute TestScore and append it to CASchools
data$score &lt;- (data$outpatient_1 + data$outpatient_2)/2     </code></pre>
<p>If we ran <tt>head(data)</tt> again we would find the two variables of interest as additional columns named <tt>PDR</tt> and <tt>score</tt> (check this!).</p>
<p>There are several functions which can be used to summarize the distribution of patient scores and patient-doctor ratios, e.g.,</p>
<ul>
<li><p><tt>mean()</tt> (computes the arithmetic mean of the provided numbers),</p></li>
<li><p><tt>sd()</tt> (computes the sample standard deviation),</p></li>
<li><p><tt>quantile()</tt> (returns a vector of the specified sample quantiles for the data).</p></li>
</ul>
<p>The next code chunk shows how to achieve this. First, we compute summary statistics on the columns <tt>PDR</tt> and <tt>score</tt> of <tt>data</tt>. In order to get nice output we gather the measures in a <tt>data.frame</tt> named <tt>DistributionSummary</tt>.</p>
<pre class="r"><code># compute sample averages of STR and score
avg_PDR &lt;- mean(data$PDR) 
avg_score &lt;- mean(data$score)
# compute sample standard deviations of STR and score
sd_PDR &lt;- sd(data$PDR) 
sd_score &lt;- sd(data$score)
# set up a vector of percentiles and compute the quantiles 
quantiles &lt;- c(0.10, 0.25, 0.4, 0.5, 0.6, 0.75, 0.9)
quant_PDR &lt;- quantile(data$PDR, quantiles)
quant_score &lt;- quantile(data$score, quantiles)
# gather everything in a data.frame 
DistributionSummary &lt;- data.frame(Average = c(avg_PDR, avg_score), 
                                  StandardDeviation = c(sd_PDR, sd_score), 
                                  quantile = rbind(quant_PDR, quant_score))
# print the summary to the console
DistributionSummary</code></pre>
<pre><code>##               Average StandardDeviation quantile.10. quantile.25.
## quant_PDR    19.64043          1.891812      17.3486     18.58236
## quant_score 654.15655         19.053347     630.3950    640.05000
##             quantile.40. quantile.50. quantile.60. quantile.75.
## quant_PDR       19.26618     19.72321      20.0783     20.87181
## quant_score    649.06999    654.45000     659.4000    666.66249
##             quantile.90.
## quant_PDR       21.86741
## quant_score    678.85999</code></pre>
<p>As for the sample data, we will use <tt>plot()</tt>. This allows us to detect characteristics of our data, such as outliers which are harder to discover by looking at mere numbers. This time we add some additional arguments to the call of <tt>plot()</tt>.</p>
<p>The first argument in our call of <tt>plot()</tt>, <tt>score ~ PDR</tt>, is again a formula that states variables on the y- and the x-axis. However, this time the two variables are not saved in separate vectors but are columns of <tt>data</tt>. Therefore, <tt>R</tt> would not find them without the argument <tt>data</tt> being correctly specified. <tt>data</tt> must be in accordance with the name of the <tt>data.frame</tt> to which the variables belong to, in this case <tt>data</tt>. Further arguments are used to change the appearance of the plot: while <tt>main</tt> adds a title, <tt>xlab</tt> and <tt>ylab</tt> add custom labels to both axes.</p>
<div class="unfolded">
<pre class="r"><code>plot(score ~ PDR, 
     data = data,
     main = &quot;Scatterplot of Score and PDR&quot;, 
     xlab = &quot;PDR (X)&quot;,
     ylab = &quot;Score (Y)&quot;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-51-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>The plot shows the scatterplot of all observations on the patient-doctor ratio and patient score. We see that the points are strongly scattered, and that the variables are negatively correlated. That is, we expect to observe lower patient scores when their are more patients per doctor.</p>
<p>The function <tt>cor()</tt> (see <tt>?cor</tt> for further info) can be used to compute the correlation between two <em>numeric</em> vectors.</p>
<pre class="r"><code>cor(data$PDR, data$score)</code></pre>
<pre><code>## [1] -0.2263627</code></pre>
<p>As the scatterplot already suggests, the correlation is negative but rather weak.</p>
<p>The task we are now facing is to find a line which best fits the data. Of course we could simply stick with graphical inspection and correlation analysis and then select the best fitting line by eyeballing. However, this would be rather subjective: different observers would draw different regression lines. On this account, we are interested in techniques that are less arbitrary. Such a technique is given by ordinary least squares (OLS) estimation.</p>
<div id="the-ordinary-least-squares-estimator" class="section level3 unnumbered">
<h3>The Ordinary Least Squares Estimator</h3>
<p>The OLS estimator chooses the regression coefficients such that the estimated regression line is as “close” as possible to the observed data points. Here, closeness is measured by the sum of the squared mistakes made in predicting <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. Let <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> be some estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Then the sum of squared estimation mistakes can be expressed as</p>
<p><span class="math display">\[ \sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2. \]</span></p>
<p>The OLS estimator in the simple regression model is the pair of estimators for intercept and slope which minimizes the expression above, which is summarized in Key Concept 15.</p>
<div id="KC15" class="keyconcept">
<h3 class="right">
Key Concept 15
</h3>
<h3 class="left">
The OLS Estimator
</h3>
<p>
The OLS estimators of the slope <span class="math inline">\(\beta_1\)</span> and the intercept <span class="math inline">\(\beta_0\)</span> in the simple linear regression model are <span class="math display">\[\begin{align}
  \hat\beta_1 &amp; = \frac{ \sum_{i = 1}^n (X_i - \overline{X})(Y_i - \overline{Y}) } { \sum_{i=1}^n (X_i - \overline{X})^2},  \\
  \\
  \hat\beta_0 &amp; =  \overline{Y} - \hat\beta_1 \overline{X}. 
\end{align}\]</span> The OLS predicted values <span class="math inline">\(\widehat{Y}_i\)</span> and residuals <span class="math inline">\(\hat{u}_i\)</span> are <span class="math display">\[\begin{align}
  \widehat{Y}_i &amp; =  \hat\beta_0 + \hat\beta_1 X_i,\\
  \\
  \hat{u}_i &amp; =  Y_i - \widehat{Y}_i. 
\end{align}\]</span> The estimated intercept <span class="math inline">\(\hat{\beta}_0\)</span>, the slope parameter <span class="math inline">\(\hat{\beta}_1\)</span> and the residuals <span class="math inline">\(\left(\hat{u}_i\right)\)</span> are computed from a sample of <span class="math inline">\(n\)</span> observations of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(i\)</span>, <span class="math inline">\(...\)</span>, <span class="math inline">\(n\)</span>. These are <em>estimates</em> of the unknown population intercept <span class="math inline">\(\left(\beta_0 \right)\)</span>, slope <span class="math inline">\(\left(\beta_1\right)\)</span>, and error term <span class="math inline">\((u_i)\)</span>.
</p>
<p>The formulas presented above may not be very intuitive at first glance. The following interactive application aims to help you understand the mechanics of OLS. You can add observations by clicking into the coordinate system where the data are represented by points. Once two or more observations are available, the application computes a regression line using OLS and some statistics which are displayed in the right panel. The results are updated as you add further observations to the left panel. A double-click resets the application, i.e., all data are removed. <iframe height="410" width="900" frameborder="0" scrolling="no" src="SimpleRegression.html"></iframe></p>
</div>
<p>There are many possible ways to compute <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> in <tt>R</tt>. For example, we could implement the formulas presented in Key Concept 15 with two of <tt>R</tt>’s most basic functions: <tt>mean()</tt> and <tt>sum()</tt>. Before doing so we <em>attach</em> the <tt>data</tt> dataset.</p>
<pre class="r"><code>attach(data) # allows to use the variables contained in CASchools directly
# compute beta_1_hat
beta_1 &lt;- sum((PDR - mean(PDR)) * (score - mean(score))) / sum((PDR - mean(PDR))^2)
# compute beta_0_hat
beta_0 &lt;- mean(score) - beta_1 * mean(PDR)
# print the results to the console
beta_1</code></pre>
<pre><code>## [1] -2.279808</code></pre>
<pre class="r"><code>beta_0</code></pre>
<pre><code>## [1] 698.9329</code></pre>

<div class="rmdknit">
Calling <tt>attach(data)</tt> enables us to adress a variable contained in <tt>data)</tt> by its name: it is no longer necessary to use the <tt>$</tt> operator in conjunction with the dataset: <tt>R</tt> may evaluate the variable name directly. <tt>R</tt> uses the object in the user environment if this object shares the name of variable contained in an attached database. However, it is a better practice to always use distinctive names in order to avoid such (seemingly) ambivalences!
</div>

<p><br></p>
<p><strong>Note that we adress variables contained in the attached dataset <tt>data</tt> directly for the rest of this tutorial!</strong></p>
<p>Of course, there are even more manual ways to perform these tasks. With OLS being one of the most widely-used estimation techniques, <tt>R</tt> of course already contains a built-in function named <tt>lm()</tt> (<strong>l</strong>inear <strong>m</strong>odel) which can be used to carry out regression analysis.</p>
<p>The first argument of the function to be specified is, similar to <tt>plot()</tt>, the regression formula with the basic syntax <tt>y ~ x</tt> where <tt>y</tt> is the dependent variable and <tt>x</tt> the explanatory variable. The argument <tt>data</tt> determines the data set to be used in the regression. We now analyze the relationship between the patient scores and the patient-doctor ratio is analyzed, using <tt>lm()</tt>.</p>
<pre class="r"><code># estimate the model and assign the result to linear_model
linear_model &lt;- lm(score ~ PDR, data = data)
# print the standard output of the estimated lm object to the console 
linear_model</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ PDR, data = data)
## 
## Coefficients:
## (Intercept)          PDR  
##      698.93        -2.28</code></pre>
<p>Let us add the estimated regression line to the plot. This time we also enlarge the ranges of both axes by setting the arguments <tt>xlim</tt> and <tt>ylim</tt>.</p>
<div class="unfolded">
<pre class="r"><code># plot the data
plot(score ~ PDR, 
     data = data,
     main = &quot;Scatterplot of Score and PDR&quot;, 
     xlab = &quot;PDR (X)&quot;,
     ylab = &quot;Score (Y)&quot;,
     xlim = c(10, 30),
     ylim = c(600, 720))
# add the regression line
abline(linear_model) </code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-57-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>Did you notice that this time, we did not pass the intercept and slope parameters to <tt>abline</tt>? If you call <tt>abline()</tt> on an object of class <tt>lm</tt> which only contains a single regressor, <tt>R</tt> draws the regression line automatically!</p>
</div>
</div>
<div id="measures-of-fit" class="section level2">
<h2><span class="header-section-number">1.9</span> Measures of Fit</h2>
<p>After fitting a linear regression model, a natural question is how well the model describes the data. Visually, this amounts to assessing whether the observations are tightly clustered around the regression line. Both the <em>coefficient of determination</em> and the <em>standard error of the regression</em> measure how well the OLS Regression line fits the data.</p>
<div id="the-coefficient-of-determination" class="section level3 unnumbered">
<h3>The Coefficient of Determination</h3>
<p><span class="math inline">\(R^2\)</span>, the <em>coefficient of determination</em>, is the fraction of the sample variance of <span class="math inline">\(Y_i\)</span> that is explained by <span class="math inline">\(X_i\)</span>. Mathematically, the <span class="math inline">\(R^2\)</span> can be written as the ratio of the explained sum of squares to the total sum of squares. The <em>explained sum of squares</em> (<span class="math inline">\(ESS\)</span>) is the sum of squared deviations of the predicted values <span class="math inline">\(\hat{Y_i}\)</span>, from the average of the <span class="math inline">\(Y_i\)</span>. The <em>total sum of squares</em> (<span class="math inline">\(TSS\)</span>) is the sum of squared deviations of the <span class="math inline">\(Y_i\)</span> from their average. Thus we have</p>
<p><span class="math display">\[\begin{align}
  ESS &amp; =  \sum_{i = 1}^n \left( \hat{Y_i} - \overline{Y} \right)^2,   \\
  TSS &amp; =  \sum_{i = 1}^n \left( Y_i - \overline{Y} \right)^2,   \\
  R^2 &amp; = \frac{ESS}{TSS}.
\end{align}\]</span></p>
<p>Since <span class="math inline">\(TSS = ESS + SSR\)</span> we can also write <span class="math display">\[ R^2 = 1- \frac{SSR}{TSS} \]</span></p>
<p>where <span class="math inline">\(SSR\)</span> is the sum of squared residuals, a measure for the errors made when predicting the <span class="math inline">\(Y\)</span> by <span class="math inline">\(X\)</span>. The <span class="math inline">\(SSR\)</span> is defined as <span class="math display">\[ SSR = \sum_{i=1}^n \hat{u}_i^2. \]</span></p>
<p><span class="math inline">\(R^2\)</span> lies between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. It is easy to see that a perfect fit, i.e., no errors made when fitting the regression line, implies <span class="math inline">\(R^2 = 1\)</span> since then we have <span class="math inline">\(SSR=0\)</span>. On the contrary, if our estimated regression line does not explain any variation in the <span class="math inline">\(Y_i\)</span>, we have <span class="math inline">\(ESS=0\)</span> and consequently <span class="math inline">\(R^2=0\)</span>.</p>
</div>
<div id="the-standard-error-of-the-regression" class="section level3 unnumbered">
<h3>The Standard Error of the Regression</h3>
<p>The <em>Standard Error of the Regression</em> (<span class="math inline">\(SER\)</span>) is an estimator of the standard deviation of the residuals <span class="math inline">\(\hat{u}_i\)</span>. As such it measures the magnitude of a typical deviation from the regression line, i.e., the magnitude of a typical residual.</p>
<p><span class="math display">\[ SER = s_{\hat{u}} = \sqrt{s_{\hat{u}}^2} \ \ \ \text{where} \ \ \ s_{\hat{u} }^2 = \frac{1}{n-2} \sum_{i = 1}^n \hat{u}^2_i = \frac{SSR}{n - 2} \]</span></p>
<p>Remember that the <span class="math inline">\(u_i\)</span> are <em>unobserved</em>. This is why we use their estimated counterparts, the residuals <span class="math inline">\(\hat{u}_i\)</span>, instead.</p>
</div>
<div id="application-to-the-hospital-data" class="section level3 unnumbered">
<h3>Application to Hospital Data</h3>
Both measures of fit can be obtained by using the function <tt>summary()</tt> with an <tt>lm</tt> object provided as the only argument. While the function <tt>lm()</tt> only prints out the estimated coefficients to the console, <tt>summary()</tt> provides additional predefined information such as the regression’s <span class="math inline">\(R^2\)</span> and the <span class="math inline">\(SER\)</span>.<br />

<div class="unfolded">
<pre class="r"><code>mod_summary &lt;- summary(linear_model)
mod_summary</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ PDR, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -47.727 -14.251   0.483  12.822  48.540 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 698.9329     9.4675  73.825  &lt; 2e-16 ***
## PDR          -2.2798     0.4798  -4.751 2.78e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18.58 on 418 degrees of freedom
## Multiple R-squared:  0.05124,    Adjusted R-squared:  0.04897 
## F-statistic: 22.58 on 1 and 418 DF,  p-value: 2.783e-06</code></pre>
</div>
<p>The <span class="math inline">\(R^2\)</span> in the output is called <em>Multiple R-squared</em> and has a value of <span class="math inline">\(0.051\)</span>. Hence, <span class="math inline">\(5.1 \%\)</span> of the variance of the dependent variable <span class="math inline">\(score\)</span> is explained by the explanatory variable <span class="math inline">\(PDR\)</span>. That is, the regression explains little of the variance in <span class="math inline">\(score\)</span>, and much of the variation in patient scores remains unexplained.</p>
<p>The <span class="math inline">\(SER\)</span> is called <em>Residual standard error</em> and equals <span class="math inline">\(18.58\)</span>. The unit of the <span class="math inline">\(SER\)</span> is the same as the unit of the dependent variable. That is, on average the deviation of the actual patient score and the regression line is <span class="math inline">\(18.58\)</span> points. Now, let us check whether <tt>summary()</tt> uses the same definitions for <span class="math inline">\(R^2\)</span> and <span class="math inline">\(SER\)</span> as we do when computing them manually.</p>
<pre class="r"><code># compute R^2 manually
SSR &lt;- sum(mod_summary$residuals^2)
TSS &lt;- sum((score - mean(score))^2)
R2 &lt;- 1 - SSR/TSS
# print the value to the console
R2</code></pre>
<pre><code>## [1] 0.05124009</code></pre>
<pre class="r"><code># compute SER manually
n &lt;- nrow(data)
SER &lt;- sqrt(SSR / (n-2))
# print the value to the console
SER</code></pre>
<pre><code>## [1] 18.58097</code></pre>
<p>We find that the results coincide. Note that the values provided by <tt>summary()</tt> are rounded to two decimal places.</p>
</div>
</div>
<div id="tlsa" class="section level2">
<h2><span class="header-section-number">1.10</span> The Least Squares Assumptions</h2>
<p>OLS performs well under a quite broad variety of different circumstances. However, there are some assumptions which need to be satisfied in order to ensure that the estimates are normally distributed in large samples (we discuss this in Chapter @ref(tsdotoe).</p>
<div id="KC16" class="keyconcept">
<h3 class="right">
Key Concept 16
</h3>
<h3 class="left">
The Least Squares Assumptions
</h3>
<p>
<span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + u_i \text{, } i = 1,\dots,n\]</span> where 1. The error term <span class="math inline">\(u_i\)</span> has conditional mean zero given <span class="math inline">\(X_i\)</span>: <span class="math inline">\(E(u_i|X_i) = 0\)</span>. 2. <span class="math inline">\((X_i,Y_i), i = 1,\dots,n\)</span> are independent and identically distributed (i.i.d.) draws from their joint distribution. 3. Large outliers are unlikely: <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> have nonzero finite fourth moments.
</p>
</div>
<div id="assumption-1-the-error-term-has-conditional-mean-of-zero" class="section level3 unnumbered">
<h3>Assumption 1: The Error Term has Conditional Mean of Zero</h3>
<p>This means that no matter which value we choose for <span class="math inline">\(X\)</span>, the error term <span class="math inline">\(u\)</span> must not show any systematic pattern and must have a mean of <span class="math inline">\(0\)</span>. Consider the case that, unconditionally, <span class="math inline">\(E(u) = 0\)</span>, but for low and high values of <span class="math inline">\(X\)</span>, the error term tends to be positive and for midrange values of <span class="math inline">\(X\)</span> the error tends to be negative. We can use R to construct such an example. To do so we generate our own data using <tt>R</tt>’s built-in random number generators.</p>
<p>We will use the following functions: * <tt>runif()</tt> - generates uniformly distributed random numbers * <tt>rnorm()</tt> - generates normally distributed random numbers * <tt>predict()</tt> - does predictions based on the results of model fitting functions like <tt>lm()</tt> * <tt>lines()</tt> - adds line segments to an existing plot</p>
<p>We start by creating a vector containing values that are uniformly distributed on the interval <span class="math inline">\([-5,5]\)</span>. This can be done with the function <tt>runif()</tt>. We also need to simulate the error term. For this we generate normally distributed random numbers with a mean equal to <span class="math inline">\(0\)</span> and a variance of <span class="math inline">\(1\)</span> using <tt>rnorm()</tt>. The <span class="math inline">\(Y\)</span> values are obtained as a quadratic function of the <span class="math inline">\(X\)</span> values and the error.</p>
After generating the data we estimate both a simple regression model and a quadratic model that also includes the regressor <span class="math inline">\(X^2\)</span> (this is a multiple regression model, see next lecture). Finally, we plot the simulated data and add the estimated regression line of a simple regression model as well as the predictions made with a quadratic model to compare the fit graphically.
<div class="unfolded">
<pre class="r"><code># set a seed to make the results reproducible
set.seed(321)
# simulate the data 
X &lt;- runif(50, min = -5, max = 5)
u &lt;- rnorm(50, sd = 5)  
# the true relation  
Y &lt;- X^2 + 2 * X + u                
# estimate a simple regression model 
mod_simple &lt;- lm(Y ~ X)
# predict using a quadratic model 
prediction &lt;- predict(lm(Y ~ X + I(X^2)), data.frame(X = sort(X)))
# plot the results
plot(Y ~ X)
abline(mod_simple, col = &quot;red&quot;)
lines(sort(X), prediction)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-62-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>The plot shows what is meant by <span class="math inline">\(E(u_i|X_i) = 0\)</span> and why it does not hold for the linear model:</p>
<p>Using the quadratic model (represented by the black curve) we see that there are no systematic deviations of the observation from the predicted relation. It is credible that the assumption is not violated when such a model is employed. However, using a simple linear regression model we see that the assumption is probably violated as <span class="math inline">\(E(u_i|X_i)\)</span> varies with the <span class="math inline">\(X_i\)</span>.</p>
</div>
<div id="assumption-2-independently-and-identically-distributed-data" class="section level3 unnumbered">
<h3>Assumption 2: Independently and Identically Distributed Data</h3>
<p>Most sampling schemes used when collecting data from populations produce i.i.d.-samples. For example, we could use <tt>R</tt>’s random number generator to randomly select student IDs from a university’s enrollment list and record age <span class="math inline">\(X\)</span> and earnings <span class="math inline">\(Y\)</span> of the corresponding students. This is a typical example of simple random sampling and ensures that all the <span class="math inline">\((X_i, Y_i)\)</span> are drawn randomly from the same population.</p>
<p>A prominent example where the i.i.d. assumption is not fulfilled is time series data where we have observations on the same unit over time. For example, take <span class="math inline">\(X\)</span> as patient health over time. Due to the aging process health declines with time, but there are also some non-deterministic influences that relate to environment, diet, lifestyle, etc. Using <tt>R</tt> we can easily simulate such a process and plot it.</p>
<p>We start the series with a total of 5000 patients and simulate the reduction in health with an autoregressive process that exhibits a downward movement in the long-run and has normally distributed errors. We cover more on autoregressive processes and time series analysis in general in the Advanced Data Analysis course. <span class="math display">\[ health_t = -5 + 0.98 \cdot health_{t-1} + u_t \]</span></p>
<div class="unfolded">
<pre class="r"><code># set seed
set.seed(123)
# generate a date vector
Date &lt;- seq(as.Date(&quot;1951/1/1&quot;), as.Date(&quot;2031/1/1&quot;), &quot;years&quot;)
# initialize the health vector
X &lt;- c(5000, rep(NA, length(Date)-1))
# generate time series observations with random influences
for (i in 2:length(Date)) {
  
    X[i] &lt;- -50 + 0.98 * X[i-1] + rnorm(n = 1, sd = 200)
    
}
#plot the results
plot(x = Date, 
     y = X, 
     type = &quot;l&quot;, 
     col = &quot;steelblue&quot;, 
     ylab = &quot;Health metric&quot;, 
     xlab = &quot;Time&quot;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-63-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>It is evident that the observations on patient health status cannot be independent in this example: the level of today’s health is correlated with tomorrows health. Thus, the i.i.d. assumption is violated.</p>
</div>
<div id="assumption-3-large-outliers-are-unlikely" class="section level3 unnumbered">
<h3>Assumption 3: Large Outliers are Unlikely</h3>
<p>It is easy to come up with situations where extreme observations, i.e., observations that deviate considerably from the usual range of the data, may occur. Such observations are called outliers. Technically speaking, assumption 3 requires that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a finite kurtosis.</p>
<p>Common cases where we want to exclude or (if possible) correct such outliers is when they are apparently typos, conversion errors or measurement errors. Even if it seems like extreme observations have been recorded correctly, it is advisable to exclude them before estimating a model since OLS suffers from <em>sensitivity to outliers</em>.</p>
<p>What does this mean? One can show that extreme observations receive heavy weighting in the estimation of the unknown regression coefficients when using OLS. Therefore, outliers can lead to strongly distorted estimates of regression coefficients. To get a better impression of this issue, consider the following application where we have placed some sample data on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> which are highly correlated. The relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> seems to be explained pretty well by the plotted regression line: all of the white data points lie close to the red regression line and we have <span class="math inline">\(R^2=0.92\)</span>.</p>
<p>Now go ahead and add a further observation at, say, <span class="math inline">\((18,2)\)</span>. This observations clearly is an outlier. The result is quite striking: the estimated regression line differs greatly from the one we adjudged to fit the data well. The slope is heavily downward biased and <span class="math inline">\(R^2\)</span> decreased to a mere <span class="math inline">\(29\%\)</span>! <br></p>
<p>Double-click inside the coordinate system to reset the app. Feel free to experiment. Choose different coordinates for the outlier or add additional ones.</p>
<iframe height="410" width="900" frameborder="0" scrolling="no" src="Outlier.html">
</iframe>
As done above we use sample data generated using <tt>R</tt>’s random number functions <tt>rnorm()</tt> and <tt>runif()</tt>. We estimate two simple regression models, one based on the original data set and another using a modified set where one observation is change to be an outlier and then plot the results. In order to understand the complete code you should be familiar with the function <tt>sort()</tt> which sorts the entries of a numeric vector in ascending order.
<div class="unfolded">
<pre class="r"><code># set seed
set.seed(123)
# generate the data
X &lt;- sort(runif(10, min = 30, max = 70))
Y &lt;- rnorm(10 , mean = 200, sd = 50)
Y[9] &lt;- 2000
# fit model with outlier
fit &lt;- lm(Y ~ X)
# fit model without outlier
fitWithoutOutlier &lt;- lm(Y[-9] ~ X[-9])
# plot the results
plot(Y ~ X)
abline(fit)
abline(fitWithoutOutlier, col = &quot;red&quot;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-64-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="tsdotoe" class="section level2">
<h2><span class="header-section-number">1.11</span> OLS Estimator Sampling Distribution</h2>
<p>Because <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are computed from a sample, the estimators themselves are random variables with a probability distribution — the so-called sampling distribution of the estimators — which describes the values they could take on over different samples. Although the sampling distribution of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> can be complicated when the sample size is small and generally changes with the number of observations, <span class="math inline">\(n\)</span>, it is possible, provided the assumptions discussed in the book are valid, to make certain statements about it that hold for all <span class="math inline">\(n\)</span>. In particular <span class="math display">\[ E(\hat{\beta}_0) = \beta_0 \ \ \text{and} \ \  E(\hat{\beta}_1) = \beta_1,\]</span> that is, <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are unbiased estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the true parameters. If the sample is sufficiently large, by the central limit theorem the <em>joint</em> sampling distribution of the estimators is well approximated by the bivariate normal distribution (<a href="#mjx-eqn-2.1">2.1</a>). This implies that the marginal distributions are also normal in large samples. Core facts on the large-sample distributions of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are presented in Key Concept 17.</p>
<div id="KC17" class="keyconcept">
<h3 class="right">
Key Concept 17
</h3>
<h3 class="left">
Large Sample Distribution of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>
</h3>
<p>
If the least squares assumptions hold, then in large samples <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> have a joint normal sampling distribution. The large sample normal distribution of <span class="math inline">\(\hat\beta_1\)</span> is <span class="math inline">\(\mathcal{N}(\beta_1, \sigma^2_{\hat\beta_1})\)</span>, where the variance of the distribution, <span class="math inline">\(\sigma^2_{\hat\beta_1}\)</span>, is <span class="math display">\[\begin{align}
\sigma^2_{\hat\beta_1} = \frac{1}{n} \frac{Var \left[ \left(X_i - \mu_X \right) u_i  \right]}  {\left[  Var \left(X_i \right)  \right]^2}.
\end{align}\]</span> The large sample normal distribution of <span class="math inline">\(\hat\beta_0\)</span> is <span class="math inline">\(\mathcal{N}(\beta_0, \sigma^2_{\hat\beta_0})\)</span> with <span class="math display">\[\begin{align}
\sigma^2_{\hat\beta_0} =  \frac{1}{n} \frac{Var \left( H_i u_i \right)}{ \left[  E \left(H_i^2  \right)  \right]^2 } \ , \ \text{where} \ \ H_i = 1 - \left[ \frac{\mu_X} {E \left( X_i^2\right)} \right] X_i. 
\end{align}\]</span> The interactive simulation below continuously generates random samples <span class="math inline">\((X_i,Y_i)\)</span> of <span class="math inline">\(200\)</span> observations where <span class="math inline">\(E(Y\vert X) = 100 + 3X\)</span>, estimates a simple regression model, stores the estimate of the slope <span class="math inline">\(\beta_1\)</span> and visualizes the distribution of the <span class="math inline">\(\widehat{\beta}_1\)</span>s observed so far using a histogram. The idea here is that for a large number of <span class="math inline">\(\widehat{\beta}_1\)</span>s, the histogram gives a good approximation of the sampling distribution of the estimator. By decreasing the time between two sampling iterations, it becomes clear that the shape of the histogram approaches the characteristic bell shape of a normal distribution centered at the true slope of <span class="math inline">\(3\)</span>. <iframe height="470" width="700" frameborder="0" scrolling="no" src="SmallSampleDIstReg.html"></iframe>
</p>
</div>
<div id="simulation-study-1" class="section level3 unnumbered">
<h3>Simulation Study 1</h3>
<p>Whether the statements of Key Concept 17 really hold can also be verified using <tt>R</tt>. For this we first we build our own population of <span class="math inline">\(100000\)</span> observations in total. To do this we need values for the independent variable <span class="math inline">\(X\)</span>, for the error term <span class="math inline">\(u\)</span>, and for the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. With these combined in a simple regression model, we compute the dependent variable <span class="math inline">\(Y\)</span>. <br> In our example we generate the numbers <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i = 1\)</span>, … ,<span class="math inline">\(100000\)</span> by drawing a random sample from a uniform distribution on the interval <span class="math inline">\([0,20]\)</span>. The realizations of the error terms <span class="math inline">\(u_i\)</span> are drawn from a standard normal distribution with parameters <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma^2 = 100\)</span> (note that <tt>rnorm()</tt> requires <span class="math inline">\(\sigma\)</span> as input for the argument <tt>sd</tt>, see <tt>?rnorm</tt>). Furthermore we chose <span class="math inline">\(\beta_0 = -2\)</span> and <span class="math inline">\(\beta_1 = 3.5\)</span> so the true model is <span class="math display">\[ Y_i = -2 + 3.5 \cdot X_i. \]</span> Finally, we store the results in a data.frame.</p>
<pre class="r"><code># simulate data
N &lt;- 100000
X &lt;- runif(N, min = 0, max = 20)
u &lt;- rnorm(N, sd = 10)
# population regression
Y &lt;- -2 + 3.5 * X + u
population &lt;- data.frame(X, Y)</code></pre>
<p>From now on we will consider the previously generated data as the true population (which of course would be <em>unknown</em> in a real world application, otherwise there would be no reason to draw a random sample in the first place). The knowledge about the true population and the true relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> can be used to verify the statements made in Key Concept 17. First, let us calculate the true variances <span class="math inline">\(\sigma^2_{\hat{\beta}_0}\)</span> and <span class="math inline">\(\sigma^2_{\hat{\beta}_1}\)</span> for a randomly drawn sample of size <span class="math inline">\(n = 100\)</span>.</p>
<pre class="r"><code># set sample size
n &lt;- 100
# compute the variance of beta_hat_0
H_i &lt;- 1 - mean(X) / mean(X^2) * X
var_b0 &lt;- var(H_i * u) / (n * mean(H_i^2)^2 )
# compute the variance of hat_beta_1
var_b1 &lt;- var( ( X - mean(X) ) * u ) / (100 * var(X)^2)</code></pre>
<pre class="r"><code># print variances to the console
var_b0</code></pre>
<pre><code>## [1] 4.045066</code></pre>
<pre class="r"><code>var_b1</code></pre>
<pre><code>## [1] 0.03018694</code></pre>
<p>Now let us assume that we do not know the true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and that it is not possible to observe the whole population. However, we can observe a random sample of <span class="math inline">\(n\)</span> observations. Then, it would not be possible to compute the true parameters but we could obtain estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> from the sample data using OLS. However, we know that these estimates are outcomes of random variables themselves since the observations are randomly sampled from the population. Key Concept 17 describes their distributions for large <span class="math inline">\(n\)</span>. When drawing a single sample of size <span class="math inline">\(n\)</span> it is not possible to make any statement about these distributions. Things change if we repeat the sampling scheme many times and compute the estimates for each sample: using this procedure we simulate outcomes of the respective distributions.</p>
<p>To achieve this in R, we employ the following approach: - We assign the number of repetitions, say <span class="math inline">\(10000\)</span>, to <tt>reps</tt> and then initialize a matrix <tt>fit</tt> were the estimates obtained in each sampling iteration shall be stored row-wise. Thus <tt>fit</tt> has to be a matrix of dimensions <tt>reps</tt><span class="math inline">\(\times2\)</span>. - In the next step we draw <tt>reps</tt> random samples of size <tt>n</tt> from the population and obtain the OLS estimates for each sample. The results are stored as row entries in the outcome matrix <tt>fit</tt>. This is done using a <tt>for()</tt> loop. - At last, we estimate variances of both estimators using the sampled outcomes and plot histograms of the latter. We also add a plot of the density functions belonging to the distributions that follow from Key Concept 17. The function <tt>bquote()</tt> is used to obtain math expressions in the titles and labels of both plots. See <tt>?bquote</tt>.</p>
<pre class="r"><code># set repetitions and sample size
n &lt;- 100
reps &lt;- 10000
# initialize the matrix of outcomes
fit &lt;- matrix(ncol = 2, nrow = reps)
# loop sampling and estimation of the coefficients
for (i in 1:reps){
  
 sample &lt;- population[sample(1:N, n), ]
 fit[i, ] &lt;- lm(Y ~ X, data = sample)$coefficients
 
}
# compute variance estimates using outcomes
var(fit[, 1])</code></pre>
<pre><code>## [1] 4.186832</code></pre>
<pre class="r"><code>var(fit[, 2])</code></pre>
<pre><code>## [1] 0.03096199</code></pre>
<div class="unfolded">
<pre class="r"><code># divide plotting area as 1-by-2 array
par(mfrow = c(1, 2))
# plot histograms of beta_0 estimates
hist(fit[, 1],
     cex.main = 1,
     main = bquote(The ~ Distribution  ~ of ~ 10000 ~ beta[0] ~ Estimates), 
     xlab = bquote(hat(beta)[0]), 
     freq = F)
# add true distribution to plot
curve(dnorm(x, 
            -2, 
            sqrt(var_b0)), 
      add = T, 
      col = &quot;darkred&quot;)
# plot histograms of beta_hat_1 
hist(fit[, 2],
    cex.main = 1,
     main = bquote(The ~ Distribution  ~ of ~ 10000 ~ beta[1] ~ Estimates), 
     xlab = bquote(hat(beta)[1]), 
     freq = F)
# add true distribution to plot
curve(dnorm(x, 
            3.5, 
            sqrt(var_b1)), 
      add = T, 
      col = &quot;darkred&quot;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-71-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>Our variance estimates support the statements made in Key Concept 17, coming close to the theoretical values. The histograms suggest that the distributions of the estimators can be well approximated by the respective theoretical normal distributions stated in Key Concept 17.</p>
</div>
<div id="simulation-study-2" class="section level3 unnumbered">
<h3>Simulation Study 2</h3>
<p>A further result implied by Key Concept 17 is that both estimators are consistent, i.e., they converge in probability to the true parameters we are interested in. This is because they are asymptotically unbiased and their variances converge to <span class="math inline">\(0\)</span> as <span class="math inline">\(n\)</span> increases.<p>
<p>We can check this by repeating the simulation above for a sequence of increasing sample sizes. This means we no longer assign the sample size but a <em>vector</em> of sample sizes: <tt>n &lt;- c(…)</tt>.<p> <br> 
<p>Let us look at the distributions of <span class="math inline">\(\beta_1\)</span>. The idea here is to add an additional call of <tt>for()</tt> to the code. This is done in order to loop over the vector of sample sizes <tt>n</tt>.<p>
<p>For each of the sample sizes we carry out the same simulation as before but plot a density estimate for the outcomes of each iteration over <tt>n</tt>. Notice that we have to change <tt>n</tt> to <tt>n[j]</tt> in the inner loop to ensure that the <tt>j</tt><span class="math inline">\(^{th}\)</span> element of <tt>n</tt> is used. In the simulation, we use sample sizes of <span class="math inline">\(100, 250, 1000\)</span> and <span class="math inline">\(3000\)</span>. Consequently we have a total of four distinct simulations using different sample sizes.<p>
<div class="unfolded">
<pre class="r"><code># set seed for reproducibility
set.seed(1)
# set repetitions and the vector of sample sizes
reps &lt;- 1000
n &lt;- c(100, 250, 1000, 3000)
# initialize the matrix of outcomes
fit &lt;- matrix(ncol = 2, nrow = reps)
# divide the plot panel in a 2-by-2 array
par(mfrow = c(2, 2))
# loop sampling and plotting
# outer loop over n
for (j in 1:length(n)) {
  
  # inner loop: sampling and estimating of the coefficients
  for (i in 1:reps){
    
    sample &lt;- population[sample(1:N, n[j]), ]
    fit[i, ] &lt;- lm(Y ~ X, data = sample)$coefficients
    
  }
  
  # draw density estimates
  plot(density(fit[ ,2]), xlim=c(2.5, 4.5), 
       col = j, 
       main = paste(&quot;n=&quot;, n[j]), 
       xlab = bquote(hat(beta)[1]))
  
}</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-72-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>We find that, as <span class="math inline">\(n\)</span> increases, the distribution of <span class="math inline">\(\hat\beta_1\)</span> concentrates around its mean, i.e., its variance decreases. Put differently, the likelihood of observing estimates close to the true value of <span class="math inline">\(\beta_1 = 3.5\)</span> grows as we increase the sample size. The same behavior can be observed if we analyze the distribution of <span class="math inline">\(\hat\beta_0\)</span> instead.</p>
</div>
<div id="simulation-study-3" class="section level3 unnumbered">
<h3>Simulation Study 3</h3>
<p>Furthermore, the variance of the OLS estimator for <span class="math inline">\(\beta_1\)</span> decreases as the variance of the <span class="math inline">\(X_i\)</span> increases. In other words, as we increase the amount of information provided by the regressor, that is, increasing <span class="math inline">\(Var(X)\)</span>, which is used to estimate <span class="math inline">\(\beta_1\)</span>, we become more confident that the estimate is close to the true value (i.e., <span class="math inline">\(Var(\hat\beta_1)\)</span> decreases).<p> <br> 
<p>We can visualize this by sampling observations <span class="math inline">\((X_i,Y_i)\)</span>, <span class="math inline">\(i=1,\dots,100\)</span> from a bivariate normal distribution with <span class="math display">\[E(X)=E(Y)=5,\]</span> <span class="math display">\[Var(X)=Var(Y)=5\]</span> and <span class="math display">\[Cov(X,Y)=4.\]</span> Formally, this is written down as<p>
  <span class="math display">\[\begin{align}
  \begin{pmatrix}
    X \\
    Y \\
  \end{pmatrix}
  \overset{i.i.d.}{\sim} &amp; \ \mathcal{N} 
  \left[
    \begin{pmatrix}
      5 \\
      5 \\
    \end{pmatrix}, \ 
    \begin{pmatrix}
      5 &amp; 4 \\
      4 &amp; 5 \\
    \end{pmatrix}
  \right].
\end{align}\]</span> To carry out the random sampling, we make use of the function <tt>mvrnorm()</tt> from the package <tt>MASS</tt> <span class="citation">[@R-MASS]</span> which allows to draw random samples from multivariate normal distributions, see <code>?mvtnorm</code>. Next, we use <tt>subset()</tt> to split the sample into two subsets such that the first set, <tt>set1</tt>, consists of observations that fulfill the condition <span class="math inline">\(\lvert X - \overline{X} \rvert &gt; 1\)</span> and the second set, <tt>set2</tt>, includes the remainder of the sample. We then plot both sets and use different colors to distinguish the observations.
<div class="unfolded">
<pre class="r"><code># load the MASS package
library(MASS)
# set seed for reproducibility
set.seed(4)
# simulate bivarite normal data
bvndata &lt;- mvrnorm(100, 
                mu = c(5, 5), 
                Sigma = cbind(c(5, 4), c(4, 5))) 
# assign column names / convert to data.frame
colnames(bvndata) &lt;- c(&quot;X&quot;, &quot;Y&quot;)
bvndata &lt;- as.data.frame(bvndata)
# subset the data
set1 &lt;- subset(bvndata, abs(mean(X) - X) &gt; 1)
set2 &lt;- subset(bvndata, abs(mean(X) - X) &lt;= 1)
# plot both data sets
plot(set1, 
     xlab = &quot;X&quot;, 
     ylab = &quot;Y&quot;, 
     pch = 19)
points(set2, 
       col = &quot;steelblue&quot;, 
       pch = 19)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-73-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>It is clear that observations that are close to the sample average of the <span class="math inline">\(X_i\)</span> have less variance than those that are farther away.<p>
<p>Now, if we were to draw a line as accurately as possible through either of the two sets it is intuitive that choosing the observations indicated by the black dots, i.e., using the set of observations which has larger variance than the blue ones, would result in a more precise line. Now, let us use OLS to estimate slope and intercept for both sets of observations. We then plot the observations along with both regression lines.<p>
<div class="unfolded">
<pre class="r"><code># estimate both regression lines
lm.set1 &lt;- lm(Y ~ X, data = set1)
lm.set2 &lt;- lm(Y ~ X, data = set2)
# plot observations
plot(set1, xlab = &quot;X&quot;, ylab = &quot;Y&quot;, pch = 19)
points(set2, col = &quot;steelblue&quot;, pch = 19)
# add both lines to the plot
abline(lm.set1, col = &quot;green&quot;)
abline(lm.set2, col = &quot;red&quot;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-74-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>Evidently, the green regression line does far better in describing data sampled from the bivariate normal distribution stated in (<a href="#mjx-eqn-4.3">4.3</a>) than the red line. This is a nice example for demonstrating why we are interested in a high variance of the regressor <span class="math inline">\(X\)</span>: more variance in the <span class="math inline">\(X_i\)</span> means more information from which the precision of the estimation benefits.</p>
</div>
</div>
</div>
<div id="htaciitslrm" class="section level1">
<h1><span class="header-section-number">2</span> Hypothesis Tests in Linear Regression</h1>
<p>We may now use our knowledge about the sampling distribution of the OLS estimator in order to make statements regarding its uncertainty.</p>
<p>These subsections cover the following topics:</p>
<ul>
<li><p>Testing Hypotheses regarding regression coefficients.</p></li>
<li><p>Confidence intervals for regression coefficients.</p></li>
<li><p>Regression when <span class="math inline">\(X\)</span> is a dummy variable.</p></li>
<li><p>Heteroskedasticity and Homoskedasticity.</p></li>
</ul>
<p>The packages <tt>scales</tt> <span class="citation">[@R-scales]</span> is required for reproduction of the code chunks presented throughout the rest of this tutorial. The package <tt>scales</tt> provides additional generic plot scaling methods. Make sure the package is installed before you proceed. The safest way to do so is by checking whether the following code chunk executes without any errors.</p>
<pre class="r"><code>library(scales)</code></pre>
<div id="testing-two-sided-hypotheses-concerning-the-slope-coefficient" class="section level2">
<h2><span class="header-section-number">2.1</span> Two-Sided Slope Hypotheses Testing</h2>
<p>Using the fact that <span class="math inline">\(\hat{\beta}_1\)</span> is approximately normally distributed in large samples (see <a href="#tsdotoe">Key Concept 17</a>), testing hypotheses about the true value <span class="math inline">\(\beta_1\)</span> can be done.</p>
<div id="KC18" class="keyconcept">
<h3 class="right">
Key Concept 18
</h3>
<h3 class="left">
General Form of the <span class="math inline">\(t\)</span>-Statistic
</h3>
<p>Remember that a general <span class="math inline">\(t\)</span>-statistic has the form <span class="math display">\[ t = \frac{\text{estimated value} - \text{hypothesized value}}{\text{standard error of the estimator}}.\]</span></p>
</div>
<div id="KC19" class="keyconcept">
<h3 class="right">
Key Concept 19
</h3>
<h3 class="left">
Testing Hypotheses regarding <span class="math inline">\(\beta_1\)</span>
</h3>
<p>For testing the hypothesis <span class="math inline">\(H_0: \beta_1 = \beta_{1,0}\)</span>, we need to perform the following steps: 1. Compute the standard error of <span class="math inline">\(\hat{\beta}_1\)</span>, <span class="math inline">\(SE(\hat{\beta}_1)\)</span> <span class="math display">\[ SE(\hat{\beta}_1) = \sqrt{ \hat{\sigma}^2_{\hat{\beta}_1} } \ \ , \ \ 
  \hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \times \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u_i}^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right]^2}.
\]</span> 2. Compute the <span class="math inline">\(t\)</span>-statistic <span class="math display">\[ t = \frac{\hat{\beta}_1 - \beta_{1,0}}{ SE(\hat{\beta}_1) }. \]</span> 3. Given a two sided alternative (<span class="math inline">\(H_1:\beta_1 \neq \beta_{1,0}\)</span>) we reject at the <span class="math inline">\(5\%\)</span> level if <span class="math inline">\(|t^{act}| &gt; 1.96\)</span> or, equivalently, if the <span class="math inline">\(p\)</span>-value is less than <span class="math inline">\(0.05\)</span>.<br><br />
Recall the definition of the <span class="math inline">\(p\)</span>-value:<br />
<span class="math display">\[\begin{align*}
    p \text{-value} =&amp; \, \text{Pr}_{H_0} \left[ \left| \frac{ \hat{\beta}_1 - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| &gt; \left|        \frac{ \hat{\beta}_1^{act} - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| \right] \\
    =&amp; \, \text{Pr}_{H_0} (|t| &gt; |t^{act}|) \\
    \approx&amp; \, 2 \cdot \Phi(-|t^{act}|)
  \end{align*}\]</span><br />
<p>The last transformation is due to the normal approximation for large samples.</p>
</div>
<p>Consider again the OLS regression output that gave us the regression line</p>
<p><span class="math display">\[ \widehat{Score} \ = \underset{(9.47)}{698.9} - \underset{(0.49)}{2.28} \times PDR \ , \ R^2=0.051 \ , \ SER=18.6. \]</span></p>
<p>Copy and execute the following code chunk if the above model object is not available in your working environment.</p>
<pre class="r"><code># load the dataset
data &lt;- read.table(&quot;hospitals.txt&quot;, header=TRUE)
# add patient-doctor ratio
data$PDR &lt;- data$patients/data$doctors
# add average patient score
data$score &lt;- (data$outpatient_1 + data$outpatient_2)/2
# estimate the model
linear_model &lt;- lm(score ~ PDR, data = data)          </code></pre>
<p>For testing a hypothesis concerning the slope parameter (the coefficient on <span class="math inline">\(PDR\)</span>), we need <span class="math inline">\(SE(\hat{\beta}_1)\)</span>, the standard error of the respective point estimator. As is common in the literature, standard errors are presented in parentheses below the point estimates.</p>
<p>Key Concept 18 reveals that it is rather cumbersome to compute the standard error and thereby the <span class="math inline">\(t\)</span>-statistic by hand. The question you should be asking yourself right now is: can we obtain these values with minimum effort using <tt>R</tt>? Yes, we can. Let us first use <tt>summary()</tt> to get a summary on the estimated coefficients in <tt>linear_model</tt>.</p>
<p><strong>Note</strong>: We keep things simple at the beginning and thus start out with simple examples that do not allow for robust inference. Standard errors that are robust to heteroskedasticity are introduced below @ref(hah) where we also demonstrate how they can be computed using <tt>R</tt>.</p>
<pre class="r"><code># print the summary of the coefficients to the console
summary(linear_model)$coefficients</code></pre>
<pre><code>##               Estimate Std. Error   t value      Pr(&gt;|t|)
## (Intercept) 698.932949  9.4674911 73.824516 6.569846e-242
## PDR          -2.279808  0.4798255 -4.751327  2.783308e-06</code></pre>
<p>The second column of the coefficients’ summary, reports <span class="math inline">\(SE(\hat\beta_0)\)</span> and <span class="math inline">\(SE(\hat\beta_1)\)</span>. Also, in the third column <tt>t value</tt>, we find <span class="math inline">\(t\)</span>-statistics <span class="math inline">\(t^{act}\)</span> suitable for tests of the separate hypotheses <span class="math inline">\(H_0: \beta_0=0\)</span> and <span class="math inline">\(H_0: \beta_1=0\)</span>. Furthermore, the output provides us with <span class="math inline">\(p\)</span>-values corresponding to both tests against the two-sided alternatives <span class="math inline">\(H_1:\beta_0\neq0\)</span> respectively <span class="math inline">\(H_1:\beta_1\neq0\)</span> in the fourth column of the table.</p>
<p>Let us have a closer look at the test of</p>
<p><span class="math display">\[H_0: \beta_1=0 \ \ \ vs. \ \ \ H_1: \beta_1 \neq 0.\]</span></p>
<p>We have <span class="math display">\[ t^{act} = \frac{-2.279808 - 0}{0.4798255} \approx - 4.75. \]</span></p>
<p>What does this tell us about the significance of the estimated coefficient? We reject the null hypothesis at the <span class="math inline">\(5\%\)</span> level of significance since <span class="math inline">\(|t^{act}| &gt; 1.96\)</span>. That is, the observed test statistic falls into the rejection region as <span class="math inline">\(p\text{-value} = 2.78\cdot 10^{-6} &lt; 0.05\)</span>. We conclude that the coefficient is significantly different from zero. In other words, we reject the hypothesis that the class size <em>has no influence</em> on the students test scores at the <span class="math inline">\(5\%\)</span> level.</p>
<p>Note that although the difference is negligible in the present case as we will see later, <tt>summary()</tt> does not perform the normal approximation but calculates <span class="math inline">\(p\)</span>-values using the <span class="math inline">\(t\)</span>-distribution instead. Generally, the degrees of freedom of the assumed <span class="math inline">\(t\)</span>-distribution are determined in the following manner:</p>
<p><span class="math display">\[ \text{DF} = n - k - 1 \]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations used to estimate the model and <span class="math inline">\(k\)</span> is the number of regressors, excluding the intercept. In our case, we have <span class="math inline">\(n=420\)</span> observations and the only regressor is <span class="math inline">\(PDR\)</span> so <span class="math inline">\(k=1\)</span>. The simplest way to determine the model degrees of freedom is</p>
<pre class="r"><code># determine residual degrees of freedom
linear_model$df.residual</code></pre>
<pre><code>## [1] 418</code></pre>
<p>Hence, for the assumed sampling distribution of <span class="math inline">\(\hat\beta_1\)</span> we have</p>
<p><span class="math display">\[\hat\beta_1 \sim t_{418}\]</span> such that the <span class="math inline">\(p\)</span>-value for a two-sided significance test can be obtained by executing the following code:</p>
<pre class="r"><code>2 * pt(-4.751327, df = 418)</code></pre>
<pre><code>## [1] 2.78331e-06</code></pre>
<p>The result is very close to the value provided by <tt>summary()</tt>. However since <span class="math inline">\(n\)</span> is sufficiently large one could just as well use the standard normal density to compute the <span class="math inline">\(p\)</span>-value:</p>
<pre class="r"><code>2 * pnorm(-4.751327)</code></pre>
<pre><code>## [1] 2.02086e-06</code></pre>
<p>The difference is indeed negligible. These findings tell us that, if <span class="math inline">\(H_0: \beta_1 = 0\)</span> is true and we were to repeat the whole process of gathering observations and estimating the model, observing a <span class="math inline">\(\hat\beta_1 \geq |-2.28|\)</span> is very unlikely!</p>
<p>Using <tt>R</tt> we may visualize how such a statement is made when using the normal approximation. Do not let the following code chunk deter you: the code is somewhat longer than the usual examples and looks unappealing but there is <strong>a lot</strong> of repetition since color shadings and annotations are added on both tails of the normal distribution. We recommend to execute the code step by step in order to see how the graph is augmented with the annotations.</p>
<div class="unfolded">
<pre class="r"><code># Plot the standard normal on the support [-6,6]
t &lt;- seq(-6, 6, 0.01)
plot(x = t, 
     y = dnorm(t, 0, 1), 
     type = &quot;l&quot;, 
     col = &quot;steelblue&quot;, 
     lwd = 2, 
     yaxs = &quot;i&quot;, 
     axes = F, 
     ylab = &quot;&quot;, 
     main = expression(&quot;Calculating the p-value of a Two-sided Test when&quot; ~ t^act ~ &quot;=-0.47&quot;), 
     cex.lab = 0.7,
     cex.main = 1)
tact &lt;- -4.75
axis(1, at = c(0, -1.96, 1.96, -tact, tact), cex.axis = 0.7)
# Shade the critical regions using polygon():
# critical region in left tail
polygon(x = c(-6, seq(-6, -1.96, 0.01), -1.96),
        y = c(0, dnorm(seq(-6, -1.96, 0.01)), 0), 
        col = &#39;orange&#39;)
# critical region in right tail
polygon(x = c(1.96, seq(1.96, 6, 0.01), 6),
        y = c(0, dnorm(seq(1.96, 6, 0.01)), 0), 
        col = &#39;orange&#39;)
# Add arrows and texts indicating critical regions and the p-value
arrows(-3.5, 0.2, -2.5, 0.02, length = 0.1)
arrows(3.5, 0.2, 2.5, 0.02, length = 0.1)
arrows(-5, 0.16, -4.75, 0, length = 0.1)
arrows(5, 0.16, 4.75, 0, length = 0.1)
text(-3.5, 0.22, 
     labels = expression(&quot;0.025&quot;~&quot;=&quot;~over(alpha, 2)),
     cex = 0.7)
text(3.5, 0.22, 
     labels = expression(&quot;0.025&quot;~&quot;=&quot;~over(alpha, 2)),
     cex = 0.7)
text(-5, 0.18, 
     labels = expression(paste(&quot;-|&quot;,t[act],&quot;|&quot;)), 
     cex = 0.7)
text(5, 0.18, 
     labels = expression(paste(&quot;|&quot;,t[act],&quot;|&quot;)), 
     cex = 0.7)
# Add ticks indicating critical values at the 0.05-level, t^act and -t^act 
rug(c(-1.96, 1.96), ticksize  = 0.145, lwd = 2, col = &quot;darkred&quot;)
rug(c(-tact, tact), ticksize  = -0.0451, lwd = 2, col = &quot;darkgreen&quot;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-85-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>The <span class="math inline">\(p\)</span>-Value is the area under the curve to left of <span class="math inline">\(-4.75\)</span> plus the area under the curve to the right of <span class="math inline">\(4.75\)</span>. As we already know from the calculations above, this value is very small.</p>
</div>
<div id="cifrc" class="section level2">
<h2><span class="header-section-number">2.2</span> Regression Coefficient Confidence Intervals</h2>
<p>As we already know, estimates of the regression coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are subject to sampling uncertainty. Therefore, we will <em>never</em> exactly estimate the true value of these parameters from sample data in an empirical application. However, we may construct confidence intervals for the intercept and the slope parameter.</p>
<p>A <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(\beta_i\)</span> has two equivalent definitions:</p>
<ul>
<li>The interval is the set of values for which a hypothesis test to the level of <span class="math inline">\(5\%\)</span> cannot be rejected.</li>
<li>The interval has a probability of <span class="math inline">\(95\%\)</span> to contain the true value of <span class="math inline">\(\beta_i\)</span>. So in <span class="math inline">\(95\%\)</span> of all samples that could be drawn, the confidence interval will cover the true value of <span class="math inline">\(\beta_i\)</span>.</li>
</ul>
<p>We also say that the interval has a confidence level of <span class="math inline">\(95\%\)</span>. The idea of the confidence interval is summarized in Key Concept 20.</p>
<div id="KC20" class="keyconcept">
<h3 class="right">
Key Concept 20
</h3>
<h3 class="left">
<span class="math inline">\(\beta_i\)</span> Confidence Interval
</h3>
<p>Imagine you could draw all possible random samples of given size. The interval that contains the true value <span class="math inline">\(\beta_i\)</span> in <span class="math inline">\(95\%\)</span> of all samples is given by the expression <span class="math display">\[ \text{CI}_{0.95}^{\beta_i} = \left[ \hat{\beta}_i - 1.96 \times SE(\hat{\beta}_i) \, , \, \hat{\beta}_i + 1.96 \times SE(\hat{\beta}_i) \right]. \]</span> Equivalently, this interval can be seen as the set of null hypotheses for which a <span class="math inline">\(5\%\)</span> two-sided hypothesis test does not reject.</p>
</div>
<div id="simulation-study-confidence-intervals" class="section level3 unnumbered">
<h3>Simulation Study: Confidence Intervals</h3>
<p>To get a better understanding of confidence intervals we conduct another simulation study. For now, assume that we have the following sample of <span class="math inline">\(n=100\)</span> observations on a single variable <span class="math inline">\(Y\)</span> where</p>
<span class="math display">\[ Y_i \overset{i.i.d}{\sim} \mathcal{N}(5,25), \ i = 1, \dots, 100.\]</span>
<div class="unfolded">
<pre class="r"><code># set seed for reproducibility
set.seed(4)
# generate and plot the sample data
Y &lt;- rnorm(n = 100, 
           mean = 5, 
           sd = 5)
plot(Y, 
     pch = 19, 
     col = &quot;steelblue&quot;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-88-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>We assume that the data is generated by the model</p>
<p><span class="math display">\[ Y_i = \mu + \epsilon_i \]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is an unknown constant and we know that <span class="math inline">\(\epsilon_i \overset{i.i.d.}{\sim} \mathcal{N}(0,25)\)</span>. In this model, the OLS estimator for <span class="math inline">\(\mu\)</span> is given by <span class="math display">\[ \hat\mu = \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i, \]</span> i.e., the sample average of the <span class="math inline">\(Y_i\)</span>. It further holds that</p>
<p><span class="math display">\[ SE(\hat\mu) = \frac{\sigma_{\epsilon}}{\sqrt{n}} = \frac{5}{\sqrt{100}} \]</span></p>
<p>A large-sample <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(\mu\)</span> is then given by</p>
<p><span class="math display">\[\begin{equation} 
CI^{\mu}_{0.95} = \left[\hat\mu - 1.96 \times \frac{5}{\sqrt{100}} \ , \ \hat\mu + 1.96 \times \frac{5}{\sqrt{100}}  \right].
\end{equation}\]</span></p>
<p>It is fairly easy to compute this interval in <tt>R</tt> by hand. The following code chunk generates a named vector containing the interval bounds:</p>
<pre class="r"><code>cbind(CIlower = mean(Y) - 1.96 * 5 / 10, CIupper = mean(Y) + 1.96 * 5 / 10)</code></pre>
<pre><code>##       CIlower  CIupper
## [1,] 4.502625 6.462625</code></pre>
<p>Knowing that <span class="math inline">\(\mu = 5\)</span> we see that, for our example data, the confidence interval covers true value.</p>
<p>As opposed to real world examples, we can use <tt>R</tt> to get a better understanding of confidence intervals by repeatedly sampling data, estimating <span class="math inline">\(\mu\)</span> and computing the confidence interval for <span class="math inline">\(\mu\)</span> as in @ref(eq:KI).</p>
<p>The procedure is as follows:</p>
<ul>
<li>We initialize the vectors <tt>lower</tt> and <tt>upper</tt> in which the simulated interval limits are to be saved. We want to simulate <span class="math inline">\(10000\)</span> intervals so both vectors are set to have this length.</li>
<li>We use a <tt>for()</tt> loop to sample <span class="math inline">\(100\)</span> observations from the <span class="math inline">\(\mathcal{N}(5,25)\)</span> distribution and compute <span class="math inline">\(\hat\mu\)</span> as well as the boundaries of the confidence interval in every iteration of the loop.</li>
<li>At last we join <tt>lower</tt> and <tt>upper</tt> in a matrix.</li>
</ul>
<pre class="r"><code># set seed
set.seed(1)
# initialize vectors of lower and upper interval boundaries
lower &lt;- numeric(10000)
upper &lt;- numeric(10000)
# loop sampling / estimation / CI
for(i in 1:10000) {
  
  Y &lt;- rnorm(100, mean = 5, sd = 5)
  lower[i] &lt;- mean(Y) - 1.96 * 5 / 10
  upper[i] &lt;- mean(Y) + 1.96 * 5 / 10
  
}
# join vectors of interval bounds in a matrix
CIs &lt;- cbind(lower, upper)</code></pre>
<p>According to Key Concept 20 we expect that the fraction of the <span class="math inline">\(10000\)</span> simulated intervals saved in the matrix <tt>CIs</tt> that contain the true value <span class="math inline">\(\mu=5\)</span> should be roughly <span class="math inline">\(95\%\)</span>. We can easily check this using logical operators.</p>
<pre class="r"><code>mean(CIs[, 1] &lt;= 5 &amp; 5 &lt;= CIs[, 2])</code></pre>
<pre><code>## [1] 0.9487</code></pre>
<p>The simulation shows that the fraction of intervals covering <span class="math inline">\(\mu=5\)</span>, i.e., those intervals for which <span class="math inline">\(H_0: \mu = 5\)</span> cannot be rejected is close to the theoretical value of <span class="math inline">\(95\%\)</span>.</p>
<p>Let us draw a plot of the first <span class="math inline">\(100\)</span> simulated confidence intervals and indicate those which <em>do not</em> cover the true value of <span class="math inline">\(\mu\)</span>. We do this via horizontal lines representing the confidence intervals on top of each other.</p>
<div class="unfolded">
<pre class="r"><code># identify intervals not covering mu
# (4 intervals out of 100)
ID &lt;- which(!(CIs[1:100, 1] &lt;= 5 &amp; 5 &lt;= CIs[1:100, 2]))
# initialize the plot
plot(0, 
     xlim = c(3, 7), 
     ylim = c(1, 100), 
     ylab = &quot;Sample&quot;, 
     xlab = expression(mu), 
     main = &quot;Confidence Intervals&quot;)
# set up color vector
colors &lt;- rep(gray(0.6), 100)
colors[ID] &lt;- &quot;red&quot;
# draw reference line at mu=5
abline(v = 5, lty = 2)
# add horizontal bars representing the CIs
for(j in 1:100) {
  
  lines(c(CIs[j, 1], CIs[j, 2]), 
        c(j, j), 
        col = colors[j], 
        lwd = 2)
  
}</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-92-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>For the first <span class="math inline">\(100\)</span> samples, the true null hypothesis is rejected in four cases so these intervals do not cover <span class="math inline">\(\mu=5\)</span>. We have indicated the intervals which lead to a rejection of the null red.</p>
<p>Let us now come back to the example of patient scores and patient-doctor ratios. The regression model is stored in <tt>linear_model</tt>. An easy way to get <span class="math inline">\(95\%\)</span> confidence intervals for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the coefficients on <tt>(intercept)</tt> and <tt>PDR</tt>, is to use the function <tt>confint()</tt>. We only have to provide a fitted model object as an input to this function. The confidence level is set to <span class="math inline">\(95\%\)</span> by default but can be modified by setting the argument <tt>level</tt>, see <tt>?confint</tt>.</p>
<pre class="r"><code># compute 95% confidence interval for coefficients in &#39;linear_model&#39;
confint(linear_model)</code></pre>
<pre><code>##                 2.5 %     97.5 %
## (Intercept) 680.32312 717.542775
## PDR          -3.22298  -1.336636</code></pre>
<p>Let us check if the calculation is done as we expect it to be for <span class="math inline">\(\beta_1\)</span>, the coefficient on <tt>STR</tt>.</p>
<pre class="r"><code># compute 95% confidence interval for coefficients in &#39;linear_model&#39; by hand
lm_summ &lt;- summary(linear_model)
c(&quot;lower&quot; = lm_summ$coef[2,1] - qt(0.975, df = lm_summ$df[2]) * lm_summ$coef[2, 2],
  &quot;upper&quot; = lm_summ$coef[2,1] + qt(0.975, df = lm_summ$df[2]) * lm_summ$coef[2, 2])</code></pre>
<pre><code>##     lower     upper 
## -3.222980 -1.336636</code></pre>
<p>The upper and the lower bounds coincide. We have used the <span class="math inline">\(0.975\)</span>-quantile of the <span class="math inline">\(t_{418}\)</span> distribution to get the exact result reported by <tt>confint</tt>. Obviously, this interval <em>does not</em> contain the value zero which, as we have already seen in the previous section, leads to the rejection of the null hypothesis <span class="math inline">\(\beta_{1,0} = 0\)</span>.</p>
</div>
</div>
<div id="rwxiabv" class="section level2">
<h2><span class="header-section-number">2.3</span> Regression with a Binary Variable</h2>
<p>Instead of using a continuous regressor <span class="math inline">\(X\)</span>, we might be interested in running the regression</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 D_i + u_i \]</span></p>
<p>where <span class="math inline">\(D_i\)</span> is a binary variable, a so-called <em>dummy variable</em>. For example, we may define <span class="math inline">\(D_i\)</span> as follows:</p>
<p><span class="math display">\[ D_i = \begin{cases}
        1 \ \ \text{if $PDR$ in $i^{th}$ hospital &lt; 20} \\
        0 \ \ \text{if $PDR$ in $i^{th}$ hospital $\geq$ 20} \\
      \end{cases} \]</span></p>
<p>The regression model now is</p>
<p><span class="math display">\[ Score_i = \beta_0 + \beta_1 D_i + u_i. \]</span></p>
<p>Let us see how these data look like in a scatter plot:</p>
<div class="unfolded">
<pre class="r"><code># Create the dummy variable as defined above
data$D &lt;- data$PDR &lt; 20
# Plot the data
plot(data$D, data$score,            # provide the data to be plotted
     pch = 20,                                # use filled circles as plot symbols
     cex = 0.5,                               # set size of plot symbols to 0.5
     col = &quot;Steelblue&quot;,                       # set the symbols&#39; color to &quot;Steelblue&quot;
     xlab = expression(D[i]),                 # Set title and axis names
     ylab = &quot;Test Score&quot;,
     main = &quot;Dummy Regression&quot;)</code></pre>
</div>
<div class="unfolded">
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-96-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>With <span class="math inline">\(D\)</span> as the regressor, it is not useful to think of <span class="math inline">\(\beta_1\)</span> as a slope parameter since <span class="math inline">\(D_i \in \{0,1\}\)</span>, i.e., we only observe two discrete values instead of a continuum of regressor values. There is no continuous line depicting the conditional expectation function <span class="math inline">\(E(Score_i | D_i)\)</span> since this function is solely defined for <span class="math inline">\(x\)</span>-positions <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p>
<p>Therefore, the interpretation of the coefficients in this regression model is as follows:</p>
<ul>
<li><p><span class="math inline">\(E(Y_i | D_i = 0) = \beta_0\)</span>, so <span class="math inline">\(\beta_0\)</span> is the expected patient score in hospitals where <span class="math inline">\(D_i=0\)</span> where <span class="math inline">\(PDR\)</span> is below <span class="math inline">\(20\)</span>.</p></li>
<li><p><span class="math inline">\(E(Y_i | D_i = 1) = \beta_0 + \beta_1\)</span> or, using the result above, <span class="math inline">\(\beta_1 = E(Y_i | D_i = 1) - E(Y_i | D_i = 0)\)</span>. Thus, <span class="math inline">\(\beta_1\)</span> is <em>the difference in group specific expectations</em>, i.e., the difference in expected patient score between hospitals with <span class="math inline">\(PDR &lt; 20\)</span> and those with <span class="math inline">\(PDR \geq 20\)</span>.</p></li>
</ul>
<p>We will now use <tt>R</tt> to estimate the dummy regression model:</p>
<div class="unfolded">
<pre class="r"><code># estimate the dummy regression model
dummy_model &lt;- lm(score ~ D, data = data)
summary(dummy_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ D, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -50.496 -14.029  -0.346  12.884  49.504 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  650.077      1.393 466.666  &lt; 2e-16 ***
## DTRUE          7.169      1.847   3.882  0.00012 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18.74 on 418 degrees of freedom
## Multiple R-squared:  0.0348, Adjusted R-squared:  0.0325 
## F-statistic: 15.07 on 1 and 418 DF,  p-value: 0.0001202</code></pre>
</div>

<div class="rmdnote">
<tt>summary()</tt> reports the <span class="math inline">\(p\)</span>-value of the test that the coefficient on <tt>(Intercept)</tt> is zero to to be <tt>&lt; 2e-16</tt>. This scientific notation states that the <span class="math inline">\(p\)</span>-value is smaller than <span class="math inline">\(\frac{2}{10^{16}}\)</span>, so a very small number. The reason for this is that computers cannot handle arbitrary small numbers. In fact, <span class="math inline">\(\frac{2}{10^{16}}\)</span> is the smallest possble number <tt>R</tt> can work with.
</div>

<p>The vector <tt>data$D</tt> has the type <tt>logical</tt> (to see this, use <tt>typeof(data$D)</tt>) which is shown in the output of <tt>summary(dummy_model)</tt>: the label <tt>DTRUE</tt> states that all entries <tt>TRUE</tt> are coded as <tt>1</tt> and all entries <tt>FALSE</tt> are coded as <tt>0</tt>. Thus, the interpretation of the coefficient <tt>DTRUE</tt> is as stated above for <span class="math inline">\(\beta_1\)</span>.</p>
<p>One can see that the expected patient score in hospitals with <span class="math inline">\(PDR &lt; 20\)</span> (<span class="math inline">\(D_i = 1\)</span>) is predicted to be <span class="math inline">\(650.1 + 7.17 = 657.27\)</span> while districts with <span class="math inline">\(PDR \geq 20\)</span> (<span class="math inline">\(D_i = 0\)</span>) are expected to have an average test score of only <span class="math inline">\(650.1\)</span>.</p>
<p>Group specific predictions can be added to the plot by execution of the following code chunk.</p>
<pre class="r"><code># add group specific predictions to the plot
points(x = data$D, 
       y = predict(dummy_model), 
       col = &quot;red&quot;, 
       pch = 20)</code></pre>
<p>Here we use the function <tt>predict()</tt> to obtain estimates of the group specific means. The red dots represent these sample group averages. Accordingly, <span class="math inline">\(\hat{\beta}_1 = 7.17\)</span> can be seen as the difference in group averages.</p>
<p><tt>summary(dummy_model)</tt> also answers the question whether there is a statistically significant difference in group means. This in turn would support the hypothesis that patient health is better when there are more doctors in hospitals. We can assess this by a two-tailed test of the hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>. Conveniently, the <span class="math inline">\(t\)</span>-statistic and the corresponding <span class="math inline">\(p\)</span>-value for this test are computed by <tt>summary()</tt>.</p>
<p>Since <tt>t value</tt> <span class="math inline">\(= 3.88 &gt; 1.96\)</span> we reject the null hypothesis at the <span class="math inline">\(5\%\)</span> level of significance. The same conclusion results when using the <span class="math inline">\(p\)</span>-value, which reports significance up to the <span class="math inline">\(0.00012\%\)</span> level.</p>
<p>As done with <tt>linear_model</tt>, we may alternatively use the function <tt>confint()</tt> to compute a <span class="math inline">\(95\%\)</span> confidence interval for the true difference in means and see if the hypothesized value is an element of this confidence set.</p>
<pre class="r"><code># confidence intervals for coefficients in the dummy regression model
confint(dummy_model)</code></pre>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) 647.338594 652.81500
## DTRUE         3.539562  10.79931</code></pre>
<p>We reject the hypothesis that there is no difference between group means at the <span class="math inline">\(5\%\)</span> significance level since <span class="math inline">\(\beta_{1,0} = 0\)</span> lies outside of <span class="math inline">\([3.54, 10.8]\)</span>, the <span class="math inline">\(95\%\)</span> confidence interval for the coefficient on <span class="math inline">\(D\)</span>.</p>
</div>
<div id="hah" class="section level2">
<h2><span class="header-section-number">2.4</span> Hetero/Homoskedasticity</h2>
<p>All inference made in the previous sections relies on the assumption that the error variance does not vary as regressor values change. But this will often not be the case in empirical applications.</p>
<div id="KC21" class="keyconcept">
<h3 class="right">
Key Concept 21
</h3>
<h3 class="left">
Hetero/Homoskedasticity
</h3>
<ul>
<li>The error term of our regression model is homoskedastic if the variance of the conditional distribution of <span class="math inline">\(u_i\)</span> given <span class="math inline">\(X_i\)</span>, <span class="math inline">\(Var(u_i|X_i=x)\)</span>, is constant <em>for all</em> observations in our sample: <span class="math display">\[ \text{Var}(u_i|X_i=x) = \sigma^2 \ \forall \ i=1,\dots,n. \]</span></li>
<li>If instead there is dependence of the conditional variance of <span class="math inline">\(u_i\)</span> on <span class="math inline">\(X_i\)</span>, the error term is said to be heteroskedastic. We then write <span class="math display">\[ \text{Var}(u_i|X_i=x) = \sigma_i^2 \ \forall \ i=1,\dots,n. \]</span></li>
<li>Homoskedasticity is a <em>special case</em> of heteroskedasticity.</li>
</ul>
</div>
<p>For a better understanding of heteroskedasticity, we generate some bivariate heteroskedastic data, estimate a linear regression model and then use box plots to depict the conditional distributions of the residuals.</p>
<div class="unfolded">
<pre class="r"><code># load scales package for adjusting color opacities
library(scales)
# generate some heteroskedastic data:
# set seed for reproducibility
set.seed(123) 
# set up vector of x coordinates
x &lt;- rep(c(10, 15, 20, 25), each = 25)
# initialize vector of errors
e &lt;- c()
# sample 100 errors such that the variance increases with x
e[1:25] &lt;- rnorm(25, sd = 10)
e[26:50] &lt;- rnorm(25, sd = 15)
e[51:75] &lt;- rnorm(25, sd = 20)
e[76:100] &lt;- rnorm(25, sd = 25)
# set up y
y &lt;- 720 - 3.3 * x + e
# Estimate the model 
mod &lt;- lm(y ~ x)
# Plot the data
plot(x = x, 
     y = y, 
     main = &quot;An Example of Heteroskedasticity&quot;,
     xlab = &quot;Patient-Doctor Ratio&quot;,
     ylab = &quot;Health Score&quot;,
     cex = 0.5, 
     pch = 19, 
     xlim = c(8, 27), 
     ylim = c(600, 710))
# Add the regression line to the plot
abline(mod, col = &quot;darkred&quot;)
# Add boxplots to the plot
boxplot(formula = y ~ x, 
        add = TRUE, 
        at = c(10, 15, 20, 25), 
        col = alpha(&quot;gray&quot;, 0.4), 
        border = &quot;black&quot;)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-102-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>We have used the <tt>formula</tt> argument <tt>y ~ x</tt> in <tt>boxplot()</tt> to specify that we want to split up the vector <tt>y</tt> into groups according to <tt>x</tt>. <tt>boxplot(y ~ x)</tt> generates a boxplot for each of the groups in <tt>y</tt> defined by <tt>x</tt>.</p>
<p>For this artificial data it is clear that the conditional error variances differ. Specifically, we observe that the variance in patient scores (and therefore the variance of the errors committed) <em>increases</em> with the patient-doctor ratio.</p>
<div id="should-we-care-about-heteroskedasticity" class="section level3 unnumbered">
<h3>Should We Care?</h3>
<p>To answer the question whether we should worry about heteroskedasticity being present, consider the variance of <span class="math inline">\(\hat\beta_1\)</span> under the assumption of homoskedasticity. In this case we have</p>
<p><span class="math display">\[ \sigma^2_{\hat\beta_1} = \frac{\sigma^2_u}{n \cdot \sigma^2_X} \]</span></p>
<p>which is a simplified version of the general equation presented in Key Concept 19. <tt>summary()</tt> estimates this by</p>
<p><span class="math display">\[ \overset{\sim}{\sigma}^2_{\hat\beta_1} = \frac{SER^2}{\sum_{i=1}^n (X_i - \overline{X})^2} \ \ \text{where} \ \ SER=\frac{1}{n-2} \sum_{i=1}^n \hat u_i^2. \]</span></p>
<p>Thus <tt>summary()</tt> estimates the <em>homoskedasticity-only</em> standard error</p>
<p><span class="math display">\[ \sqrt{ \overset{\sim}{\sigma}^2_{\hat\beta_1} } = \sqrt{ \frac{SER^2}{\sum_{i=1}^n(X_i - \overline{X})^2} }. \]</span></p>
<p>This is in fact an estimator for the standard deviation of the estimator <span class="math inline">\(\hat{\beta}_1\)</span> that is <em>inconsistent</em> for the true value <span class="math inline">\(\sigma^2_{\hat\beta_1}\)</span> when there is heteroskedasticity. The implication is that <span class="math inline">\(t\)</span>-statistics computed in the manner of Key Concept 20 do not follow a standard normal distribution, even in large samples. This issue may invalidate inference when using the previously treated tools for hypothesis testing: we should be cautious when making statements about the significance of regression coefficients on the basis of <span class="math inline">\(t\)</span>-statistics as computed by <tt>summary()</tt> or confidence intervals produced by <tt>confint()</tt> if it is doubtful for the assumption of homoskedasticity to hold!</p>
<p>We will now use <tt>R</tt> to compute the homoskedasticity-only standard error for <span class="math inline">\(\hat{\beta}_1\)</span> in the test score regression model <tt>linear_model</tt> by hand and see that it matches the value produced by <tt>summary()</tt>.</p>
<pre class="r"><code># Store model summary in &#39;model&#39;
model &lt;- summary(linear_model)
# Extract the standard error of the regression from model summary
SER &lt;- model$sigma
# Compute the variation in &#39;size&#39;
V &lt;- (nrow(data)-1) * var(data$PDR)
# Compute the standard error of the slope parameter&#39;s estimator and print it
SE.beta_1.hat &lt;- sqrt(SER^2/V)
SE.beta_1.hat</code></pre>
<pre><code>## [1] 0.4798255</code></pre>
<pre class="r"><code># Use logical operators to see if the value computed by hand matches the one provided 
# in mod$coefficients. Round estimates to four decimal places
round(model$coefficients[2, 2], 4) == round(SE.beta_1.hat, 4)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Indeed, the estimated values are equal.</p>
</div>
<div id="computation-of-heteroskedasticity-robust-standard-errors" class="section level3 unnumbered">
<h3>Robust Standard Errors</h3>
<p>Consistent estimation of <span class="math inline">\(\sigma_{\hat{\beta}_1}\)</span> under heteroskedasticity is granted when the following <em>robust</em> estimator is used.</p>
<p><span class="math display">\[ SE(\hat{\beta}_1) = \sqrt{ \frac{1}{n} \cdot \frac{ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2} } \tag{5.6} \]</span></p>
<p>Standard error estimates computed this way are also referred to as <a href="https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors">Eicker-Huber-White standard errors</a>.</p>
<p>It can be quite cumbersome to do this calculation by hand. Luckily, there are <tt>R</tt> function for that purpose. A convenient one named <tt>vcovHC()</tt> is part of the package <tt>sandwich</tt>. This function can compute a variety of standard errors.</p>
<p>Let us now compute robust standard error estimates for the coefficients in <tt>linear_model</tt>.</p>
<pre class="r"><code>library(plm)
# compute heteroskedasticity-robust standard errors
vcov &lt;- vcovHC(linear_model, type = &quot;HC1&quot;)
vcov</code></pre>
<pre><code>##             (Intercept)        PDR
## (Intercept)  107.419993 -5.3639114
## PDR           -5.363911  0.2698692</code></pre>
<p>The output of <tt>vcovHC()</tt> is the variance-covariance matrix of coefficient estimates. We are interested in the square root of the diagonal elements of this matrix, i.e., the standard error estimates.</p>

<div class="rmdknit">
When we have k &gt; 1 regressors, writing down the equations for a regression model becomes very messy. A more convinient way to denote and estimate so-called multiple regression models (see Chapter @ref(rmwmr)) is by using matrix algebra. This is why functions like <tt>vcovHC()</tt> produce matrices. In the simple linear regression model, the variances and covariances of the estimators can be gathered in the symmetric variance-covariance matrix <span class="math display">\[\begin{equation}
\text{Var}
  \begin{pmatrix}
    \hat\beta_0 \\
    \hat\beta_1
  \end{pmatrix} = 
\begin{pmatrix}
  \text{Var}(\hat\beta_0) &amp; \text{Cov}(\hat\beta_0,\hat\beta_1) \\
\text{Cov}(\hat\beta_0,\hat\beta_1) &amp; \text{Var}(\hat\beta_1)
\end{pmatrix},
\end{equation}\]</span> so <tt>vcovHC()</tt> gives us <span class="math inline">\(\widehat{\text{Var}}(\hat\beta_0)\)</span>, <span class="math inline">\(\widehat{\text{Var}}(\hat\beta_1)\)</span> and <span class="math inline">\(\widehat{\text{Cov}}(\hat\beta_0,\hat\beta_1)\)</span>, but most of the time we are interested in the diagonal elements of the estimated matrix.
</div>

<pre class="r"><code># compute the square root of the diagonal elements in vcov
robust_se &lt;- sqrt(diag(vcov))
robust_se</code></pre>
<pre><code>## (Intercept)         PDR 
##  10.3643617   0.5194893</code></pre>
<p>Now assume we want to generate a coefficient summary as provided by <tt>summary()</tt> but with <em>robust</em> standard errors of the coefficient estimators, robust <span class="math inline">\(t\)</span>-statistics and corresponding <span class="math inline">\(p\)</span>-values for the regression model <tt>linear_model</tt>. This can be done using <tt>coeftest()</tt> from the package <tt>lmtest</tt>, see <code>?coeftest</code>. Further we specify in the argument <tt>vcov.</tt> that <tt>vcov</tt>, the Eicker-Huber-White estimate of the variance matrix we have computed before, should be used.</p>
<pre class="r"><code>library(lmtest)
# we invoke the function `coeftest()` on our model
coeftest(linear_model, vcov. = vcov)</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 698.93295   10.36436 67.4362 &lt; 2.2e-16 ***
## PDR          -2.27981    0.51949 -4.3886 1.447e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We see that the values reported in the column <tt>Std. Error</tt> are equal those from <tt>sqrt(diag(vcov))</tt>.</p>
<p>How severe are the implications of using homoskedasticity-only standard errors in the presence of heteroskedasticity? The answer is: it depends. As mentioned above we face the risk of drawing wrong conclusions when conducting significance tests. <br></p>
<p>Let us illustrate this by generating another example of a heteroskedastic data set and using it to estimate a simple regression model. We take</p>
<p><span class="math display">\[ Y_i = \beta_1 \cdot X_i + u_i \ \ , \ \ u_i \overset{i.i.d.}{\sim} \mathcal{N}(0,0.36 \cdot X_i^2)  \]</span></p>
<p>with <span class="math inline">\(\beta_1=1\)</span> as the data generating process. Clearly, the assumption of homoskedasticity is violated here since the variance of the errors is a nonlinear, increasing function of <span class="math inline">\(X_i\)</span> but the errors have zero mean and are i.i.d. As before, we are interested in estimating <span class="math inline">\(\beta_1\)</span>.</p>
<pre class="r"><code># generate heteroskedastic data 
X &lt;- 1:500
Y &lt;- rnorm(n = 500, mean = X, sd = 0.6 * X)
# estimate a simple regression model
reg &lt;- lm(Y ~ X)</code></pre>
<p>We plot the data and add the regression line.</p>
<div class="unfolded">
<pre class="r"><code># plot the data
plot(x = X, y = Y, 
     pch = 19, 
     col = &quot;steelblue&quot;, 
     cex = 0.8)
# add the regression line to the plot
abline(reg, 
       col = &quot;darkred&quot;, 
       lwd = 1.5)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-108-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>The plot shows that the data are heteroskedastic as the variance of <span class="math inline">\(Y\)</span> grows with <span class="math inline">\(X\)</span>. We next conduct a significance test of the (true) null hypothesis <span class="math inline">\(H_0: \beta_1 = 1\)</span> twice, once using the homoskedasticity-only standard error formula and once with the robust version (<a href="#mjx-eqn-5.6">5.6</a>). An easy way to do this in <tt>R</tt> is the function <tt>linearHypothesis()</tt> from the package <tt>car</tt>, see <code>?linearHypothesis</code>. It allows to test linear hypotheses about parameters in linear models in a similar way as done with a <span class="math inline">\(t\)</span>-statistic and offers various robust covariance matrix estimators. We test by comparing the tests’ <span class="math inline">\(p\)</span>-values to the significance level of <span class="math inline">\(5\%\)</span>.</p>

<div class="rmdknit">
<tt>linearHypothesis()</tt> computes a test statistic that follows an <span class="math inline">\(F\)</span>-distribution under the null hypothesis. We will not loose too much words on the underlying theory. In general, the idea of the <span class="math inline">\(F\)</span>-test is to compare the fit of different models. When testing a hypothesis about a <em>single</em> coefficient using an <span class="math inline">\(F\)</span>-test, one can show that the test statistic is simply the square of the corresponding <span class="math inline">\(t\)</span>-statistic: <span class="math display">\[F = t^2 = \left(\frac{\hat\beta_i - \beta_{i,0}}{SE(\hat\beta_i)}\right)^2 \sim F_{1,n-k-1}\]</span> In <tt>linearHypothesis()</tt>, there are different ways to specify the hypothesis to be tested, e.g., using a vector of the type <tt>character</tt> (as done in the next code chunk), see <tt>?linearHypothesis</tt> for alternatives. The function returns an object of class <tt>anova</tt> which contains further information on the test that can be accessed using the <tt>$</tt> operator.
</div>

<pre class="r"><code>library(car)
# test hypthesis using the default standard error formula
linearHypothesis(reg, hypothesis.matrix = &quot;X = 1&quot;)$&#39;Pr(&gt;F)&#39;[2] &lt; 0.05</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code># test hypothesis using the robust standard error formula
linearHypothesis(reg, hypothesis.matrix = &quot;X = 1&quot;, white.adjust = &quot;hc1&quot;)$&#39;Pr(&gt;F)&#39;[2] &lt; 0.05</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>This is a good example of what can go wrong if we ignore heteroskedasticity: for the data set at hand the default method rejects the null hypothesis <span class="math inline">\(\beta_1 = 1\)</span> although it is true. When using the robust standard error formula the test does not reject the null. Of course, we could this might just be a coincidence and both tests do equally well in maintaining the type I error rate of <span class="math inline">\(5\%\)</span>. This can be further investigated by computing <em>Monte Carlo</em> estimates of the rejection frequencies of both tests on the basis of a large number of random samples. We proceed as follows:</p>
<ul>
<li>initialize vectors <tt>t</tt> and <tt>t.rob</tt>.</li>
<li>Using a <tt>for()</tt> loop, we generate <span class="math inline">\(10000\)</span> heteroskedastic random samples of size <span class="math inline">\(1000\)</span>, estimate the regression model and check whether the tests falsely reject the null at the level of <span class="math inline">\(5\%\)</span> using comparison operators. The results are stored in the respective vectors <tt>t</tt> and <tt>t.rob</tt>.</li>
<li>After the simulation, we compute the fraction of false rejections for both tests.</li>
</ul>
<pre class="r"><code># initialize vectors t and t.rob
t &lt;- c()
t.rob &lt;- c()
# loop sampling and estimation
for (i in 1:10000) {
  
  # sample data
  X &lt;- 1:1000
  Y &lt;- rnorm(n = 1000, mean = X, sd = 0.6 * X)
  # estimate regression model
  reg &lt;- lm(Y ~ X)
  # homoskedasdicity-only significance test
  t[i] &lt;- linearHypothesis(reg, &quot;X = 1&quot;)$&#39;Pr(&gt;F)&#39;[2] &lt; 0.05
  # robust significance test
  t.rob[i] &lt;- linearHypothesis(reg, &quot;X = 1&quot;, white.adjust = &quot;hc1&quot;)$&#39;Pr(&gt;F)&#39;[2] &lt; 0.05
}
# compute the fraction of false rejections
round(cbind(t = mean(t), t.rob = mean(t.rob)), 3)</code></pre>
<pre><code>##          t t.rob
## [1,] 0.073  0.05</code></pre>
<p>These results reveal the increased risk of falsely rejecting the null using the homoskedasticity-only standard error for the testing problem at hand: with the common standard error estimator, <span class="math inline">\(7.28\%\)</span> of all tests falsely reject the null hypothesis. In contrast, with the robust test statistic we are closer to the nominal level of <span class="math inline">\(5\%\)</span>.</p>
</div>
</div>
<div id="the-gauss-markov-theorem" class="section level2">
<h2><span class="header-section-number">2.5</span> The Gauss-Markov Theorem</h2>
<p>When estimating regression models, we know that the results of the estimation procedure are random. However, when using unbiased estimators, at least on average, we estimate the true parameter. When comparing different unbiased estimators, it is therefore interesting to know which one has the highest precision: being aware that the likelihood of estimating the <em>exact</em> value of the parameter of interest is <span class="math inline">\(0\)</span> in an empirical application, we want to make sure that the likelihood of obtaining an estimate very close to the true value is as high as possible. This means we want to use the estimator with the lowest variance of all unbiased estimators, provided we care about unbiasedness. The Gauss-Markov theorem states that, in the class of conditionally unbiased linear estimators, the OLS estimator has this property under certain conditions.</p>
<div id="KC22" class="keyconcept">
<h3 class="right">
Key Concept 22
</h3>
<h3 class="left">
The Gauss-Markov Theorem for <span class="math inline">\(\hat{\beta}_1\)</span>
</h3>
<p>Suppose that the assumptions hold <em>and</em> that the errors are <em>homoskedastic</em>. The OLS estimator is the best (in the sense of smallest variance) linear conditionally unbiased estimator (BLUE) in this setting. Let us have a closer look at what this means: - Estimators of <span class="math inline">\(\beta_1\)</span> that are linear functions of the <span class="math inline">\(Y_1, \dots, Y_n\)</span> and that are unbiased conditionally on the regressor <span class="math inline">\(X_1, \dots, X_n\)</span> can be written as <span class="math display">\[ \overset{\sim}{\beta}_1 = \sum_{i=1}^n a_i Y_i \]</span> where the <span class="math inline">\(a_i\)</span> are weights that are allowed to depend on the <span class="math inline">\(X_i\)</span> but <em>not</em> on the <span class="math inline">\(Y_i\)</span>. - We already know that <span class="math inline">\(\overset{\sim}{\beta}_1\)</span> has a sampling distribution: <span class="math inline">\(\overset{\sim}{\beta}_1\)</span> is a linear function of the <span class="math inline">\(Y_i\)</span> which are random variables. If now <span class="math display">\[ E(\overset{\sim}{\beta}_1 | X_1, \dots, X_n) = \beta_1, \]</span> <span class="math inline">\(\overset{\sim}{\beta}_1\)</span> is a linear unbiased estimator of <span class="math inline">\(\beta_1\)</span>, conditionally on the <span class="math inline">\(X_1, \dots, X_n\)</span>. - We may ask if <span class="math inline">\(\overset{\sim}{\beta}_1\)</span> is also the <em>best</em> estimator in this class, i.e., the most efficient one of all linear conditionally unbiased estimators where “most efficient” means smallest variance. The weights <span class="math inline">\(a_i\)</span> play an important role here and it turns out that OLS uses just the right weights to have the BLUE property.</p>
</div>
<div id="simulation-study-blue-estimator" class="section level3 unnumbered">
<h3>Simulation Study: BLUE Estimator</h3>
<p>Consider the case of a regression of <span class="math inline">\(Y_i,\dots,Y_n\)</span> only on a constant. Here, the <span class="math inline">\(Y_i\)</span> are assumed to be a random sample from a population with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. The OLS estimator in this model is simply the sample mean.</p>
<p><span class="math display">\[\begin{equation}
\hat{\beta}_1 = \sum_{i=1}^n \underbrace{\frac{1}{n}}_{=a_i} Y_i 
\end{equation}\]</span></p>
<p>Clearly, each observation is weighted by</p>
<p><span class="math display">\[a_i = \frac{1}{n}.\]</span></p>
<p>and we also know that <span class="math inline">\(\text{Var}(\hat{\beta}_1)=\frac{\sigma^2}{n}\)</span>.</p>
<p>We now use <tt>R</tt> to conduct a simulation study that demonstrates what happens to the variance if different weights <span class="math display">\[ w_i = \frac{1 \pm \epsilon}{n} \]</span> are assigned to either half of the sample <span class="math inline">\(Y_1, \dots, Y_n\)</span> instead of using <span class="math inline">\(\frac{1}{n}\)</span>, the OLS weights.</p>
<div class="unfolded">
<pre class="r"><code># set sample size and number of repetitions
n &lt;- 100      
reps &lt;- 1e5
# choose epsilon and create a vector of weights as defined above
epsilon &lt;- 0.8
w &lt;- c(rep((1 + epsilon) / n, n / 2), 
       rep((1 - epsilon) / n, n / 2) )
# draw a random sample y_1,...,y_n from the standard normal distribution, 
# use both estimators 1e5 times and store the result in the vectors &#39;ols&#39; and 
# &#39;weightedestimator&#39;
ols &lt;- rep(NA, reps)
weightedestimator &lt;- rep(NA, reps)
for (i in 1:reps) {
  
  y &lt;- rnorm(n)
  ols[i] &lt;- mean(y)
  weightedestimator[i] &lt;- crossprod(w, y)
  
}
# plot kernel density estimates of the estimators&#39; distributions: 
# OLS
plot(density(ols), 
     col = &quot;purple&quot;, 
     lwd = 3, 
     main = &quot;Density of OLS and Weighted Estimator&quot;,
     xlab = &quot;Estimates&quot;)
# weighted
lines(density(weightedestimator), 
      col = &quot;steelblue&quot;, 
      lwd = 3) 
# add a dashed line at 0 and add a legend to the plot
abline(v = 0, lty = 2)
legend(&#39;topright&#39;, 
       c(&quot;OLS&quot;, &quot;Weighted&quot;), 
       col = c(&quot;purple&quot;, &quot;steelblue&quot;), 
       lwd = 3)</code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-113-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>What conclusion can we draw from the result?</p>
<ul>
<li>Both estimators seem to be unbiased: the means of their estimated distributions are zero.</li>
<li>The estimator using weights that deviate from those implied by OLS is less efficient than the OLS estimator: there is higher dispersion when weights are <span class="math inline">\(w_i = \frac{1 \pm 0.8}{100}\)</span> instead of <span class="math inline">\(w_i=\frac{1}{100}\)</span> as required by the OLS solution.</li>
</ul>
<p>Hence, the simulation results support the Gauss-Markov Theorem.</p>
</div>
</div>
<div id="using-the-t-statistic-in-regression-when-the-sample-size-is-small" class="section level2">
<h2><span class="header-section-number">2.6</span> Small Sample t-Statistics</h2>
<p>The three OLS assumptions discussed above are the foundation for the results on the large sample distribution of the OLS estimators in the simple regression model. What can be said about the distribution of the estimators and their <span class="math inline">\(t\)</span>-statistics when the sample size is small and the population distribution of the data is unknown? Provided that the three least squares assumptions hold and the errors are normally distributed and homoskedastic (we refer to these conditions as the homoskedastic normal regression assumptions), we have normally distributed estimators and <span class="math inline">\(t\)</span>-distributed test statistics in small samples.</p>
<p>Recall the <a href="#thetdist">definition</a> of a <span class="math inline">\(t\)</span>-distributed variable</p>
<p><span class="math display">\[ \frac{Z}{\sqrt{W/M}} \sim t_M\]</span></p>
<p>where <span class="math inline">\(Z\)</span> is a standard normal random variable, <span class="math inline">\(W\)</span> is <span class="math inline">\(\chi^2\)</span> distributed with <span class="math inline">\(M\)</span> degrees of freedom and <span class="math inline">\(Z\)</span> and <span class="math inline">\(W\)</span> are independent.</p>
<p>Let us simulate the distribution of regression <span class="math inline">\(t\)</span>-statistics based on a large number of small random samples, say <span class="math inline">\(n=20\)</span>, and compare the simulated distributions to the theoretical distributions which should be <span class="math inline">\(t_{18}\)</span>, the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(18\)</span> degrees of freedom (recall that <span class="math inline">\(\text{DF}=n-k-1\)</span>).</p>
<div class="unfolded">
<pre class="r"><code># initialize two vectors
beta_0 &lt;- c()
beta_1 &lt;- c()
# loop sampling / estimation / t statistics
for (i in 1:10000) {
  
  X &lt;- runif(20, 0, 20)
  Y &lt;- rnorm(n = 20, mean = X)
  reg &lt;- summary(lm(Y ~ X))
  beta_0[i] &lt;- (reg$coefficients[1, 1] - 0)/(reg$coefficients[1, 2])
  beta_1[i] &lt;- (reg$coefficients[2, 1] - 1)/(reg$coefficients[2, 2])
  
}
# plot the distributions and compare with t_18 density:
# divide plotting area
par(mfrow = c(1, 2))
# plot the simulated density of beta_0
plot(density(beta_0), 
     lwd = 2 , 
     main = expression(widehat(beta)[0]), 
     xlim = c(-4, 4))
# add the t_18 density to the plot
curve(dt(x, df = 18), 
      add = T, 
      col = &quot;red&quot;, 
      lwd = 2, 
      lty = 2)
# plot the simulated density of beta_1
plot(density(beta_1), 
     lwd = 2, 
     main = expression(widehat(beta)[1]), xlim = c(-4, 4)
     )
# add the t_18 density to the plot
curve(dt(x, df = 18), 
      add = T, 
      col = &quot;red&quot;, 
      lwd = 2, 
      lty = 2) </code></pre>
<p><img src="basic_statistics_review_using_R_files/figure-html/unnamed-chunk-114-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>The outcomes are consistent with our expectations: the empirical distributions of both estimators seem to track the theoretical <span class="math inline">\(t_{18}\)</span> distribution quite closely.</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
